#!/usr/bin/env python3
"""
VIBA AI ì§€ì†ì  ê°œì„  ì›Œí¬í”Œë¡œìš°
============================

í…ŒìŠ¤íŠ¸ ê²°ê³¼, ì„±ëŠ¥ ë©”íŠ¸ë¦­, ì‚¬ìš©ì í”¼ë“œë°±ì„ ë¶„ì„í•˜ì—¬ 
ìë™ìœ¼ë¡œ ê°œì„  ì‚¬í•­ì„ ì‹ë³„í•˜ê³  ì‹¤í–‰í•˜ëŠ” ì‹œìŠ¤í…œ

@version 1.0
@author VIBA AI Team
@date 2025.07.06
"""

import os
import sys
import json
import time
import asyncio
import logging
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass, field
import numpy as np
import pandas as pd
from enum import Enum

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from nlp_engine.src.utils.logger import setup_logger
from nlp_engine.src.utils.metrics_collector import get_metrics_collector

logger = setup_logger(__name__)


class ImprovementPriority(Enum):
    """ê°œì„  ìš°ì„ ìˆœìœ„"""
    CRITICAL = "critical"
    HIGH = "high" 
    MEDIUM = "medium"
    LOW = "low"


class ImprovementCategory(Enum):
    """ê°œì„  ì¹´í…Œê³ ë¦¬"""
    PERFORMANCE = "performance"
    ACCURACY = "accuracy"
    USABILITY = "usability"
    RELIABILITY = "reliability"
    SCALABILITY = "scalability"
    SECURITY = "security"
    COST = "cost"


@dataclass
class ImprovementItem:
    """ê°œì„  í•­ëª©"""
    id: str
    title: str
    description: str
    category: ImprovementCategory
    priority: ImprovementPriority
    impact_score: float  # 0.0 - 1.0
    effort_score: float  # 0.0 - 1.0 (ë‚®ì„ìˆ˜ë¡ ì ì€ ë…¸ë ¥)
    roi_score: float = field(init=False)  # ìë™ ê³„ì‚°
    
    # ë©”íŠ¸ë¦­ ë° ë°ì´í„°
    current_metrics: Dict[str, float] = field(default_factory=dict)
    target_metrics: Dict[str, float] = field(default_factory=dict)
    
    # ì‹¤í–‰ ì •ë³´
    status: str = "identified"  # identified, planned, in_progress, completed, cancelled
    assigned_to: Optional[str] = None
    estimated_completion: Optional[datetime] = None
    actual_completion: Optional[datetime] = None
    
    # ì¶”ì  ì •ë³´
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    source_data: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """ROI ì ìˆ˜ ìë™ ê³„ì‚°"""
        if self.effort_score > 0:
            self.roi_score = self.impact_score / self.effort_score
        else:
            self.roi_score = 0.0


class VIBAContinuousImprovement:
    """VIBA AI ì§€ì†ì  ê°œì„  ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        ì§€ì†ì  ê°œì„  ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config or {}
        self.project_root = project_root
        
        # ë°ì´í„° ë””ë ‰í† ë¦¬
        self.data_dir = self.project_root / "improvement-data"
        self.data_dir.mkdir(exist_ok=True)
        
        # ê°œì„  í•­ëª© ì €ì¥ì†Œ
        self.improvements: List[ImprovementItem] = []
        self.improvement_history: List[Dict[str, Any]] = []
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°
        self.metrics_collector = get_metrics_collector()
        
        # ë¶„ì„ ì„¤ì •
        self.analysis_config = {
            'performance_threshold': 0.8,  # ì„±ëŠ¥ ì„ê³„ê°’
            'accuracy_threshold': 0.9,     # ì •í™•ë„ ì„ê³„ê°’
            'user_satisfaction_threshold': 0.85,  # ì‚¬ìš©ì ë§Œì¡±ë„ ì„ê³„ê°’
            'error_rate_threshold': 0.05,  # ì˜¤ë¥˜ìœ¨ ì„ê³„ê°’
            'response_time_threshold': 5.0,  # ì‘ë‹µì‹œê°„ ì„ê³„ê°’ (ì´ˆ)
            'memory_usage_threshold': 0.8,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì„ê³„ê°’
        }
        
        logger.info("VIBA ì§€ì†ì  ê°œì„  ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def run_improvement_cycle(self) -> Dict[str, Any]:
        """
        ì§€ì†ì  ê°œì„  ì‚¬ì´í´ ì‹¤í–‰
        
        Returns:
            ê°œì„  ì‚¬ì´í´ ê²°ê³¼
        """
        logger.info("ğŸ”„ ì§€ì†ì  ê°œì„  ì‚¬ì´í´ ì‹œì‘")
        
        cycle_start = time.time()
        cycle_results = {
            "cycle_id": f"improvement_{int(cycle_start)}",
            "start_time": cycle_start,
            "stages": {}
        }
        
        try:
            # 1. ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
            logger.info("ğŸ“Š 1ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„")
            analysis_result = await self._collect_and_analyze_data()
            cycle_results["stages"]["data_analysis"] = analysis_result
            
            # 2. ê°œì„  ê¸°íšŒ ì‹ë³„
            logger.info("ğŸ” 2ë‹¨ê³„: ê°œì„  ê¸°íšŒ ì‹ë³„")
            opportunities = await self._identify_improvement_opportunities(analysis_result)
            cycle_results["stages"]["opportunity_identification"] = opportunities
            
            # 3. ê°œì„  í•­ëª© ìš°ì„ ìˆœìœ„ ê²°ì •
            logger.info("ğŸ“‹ 3ë‹¨ê³„: ìš°ì„ ìˆœìœ„ ê²°ì •")
            prioritized_items = await self._prioritize_improvements(opportunities)
            cycle_results["stages"]["prioritization"] = prioritized_items
            
            # 4. ìë™ ê°œì„  ì‹¤í–‰
            logger.info("âš¡ 4ë‹¨ê³„: ìë™ ê°œì„  ì‹¤í–‰")
            auto_improvements = await self._execute_automatic_improvements(prioritized_items)
            cycle_results["stages"]["automatic_improvements"] = auto_improvements
            
            # 5. ìˆ˜ë™ ê°œì„  ê³„íš ìƒì„±
            logger.info("ğŸ“ 5ë‹¨ê³„: ìˆ˜ë™ ê°œì„  ê³„íš ìƒì„±")
            manual_plans = await self._generate_manual_improvement_plans(prioritized_items)
            cycle_results["stages"]["manual_improvement_plans"] = manual_plans
            
            # 6. ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
            logger.info("ğŸ“„ 6ë‹¨ê³„: ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±")
            report = await self._generate_improvement_report(cycle_results)
            cycle_results["final_report"] = report
            
            cycle_results["end_time"] = time.time()
            cycle_results["duration"] = cycle_results["end_time"] - cycle_start
            cycle_results["status"] = "completed"
            
            # ê²°ê³¼ ì €ì¥
            await self._save_cycle_results(cycle_results)
            
            logger.info(f"âœ… ì§€ì†ì  ê°œì„  ì‚¬ì´í´ ì™„ë£Œ ({cycle_results['duration']:.2f}ì´ˆ)")
            
            return cycle_results
            
        except Exception as e:
            logger.error(f"âŒ ì§€ì†ì  ê°œì„  ì‚¬ì´í´ ì‹¤íŒ¨: {e}")
            cycle_results["error"] = str(e)
            cycle_results["status"] = "failed"
            cycle_results["end_time"] = time.time()
            return cycle_results
    
    async def _collect_and_analyze_data(self) -> Dict[str, Any]:
        """ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„"""
        analysis_result = {
            "timestamp": time.time(),
            "data_sources": {}
        }
        
        # 1. í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
        test_results = await self._analyze_test_results()
        analysis_result["data_sources"]["test_results"] = test_results
        
        # 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„
        performance_metrics = await self._analyze_performance_metrics()
        analysis_result["data_sources"]["performance_metrics"] = performance_metrics
        
        # 3. ì‚¬ìš©ì í™œë™ ë¶„ì„
        user_activity = await self._analyze_user_activity()
        analysis_result["data_sources"]["user_activity"] = user_activity
        
        # 4. ì‹œìŠ¤í…œ ì•ˆì •ì„± ë¶„ì„
        system_stability = await self._analyze_system_stability()
        analysis_result["data_sources"]["system_stability"] = system_stability
        
        # 5. ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ë¶„ì„
        business_metrics = await self._analyze_business_metrics()
        analysis_result["data_sources"]["business_metrics"] = business_metrics
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        analysis_result["overall_health_score"] = self._calculate_overall_health_score(analysis_result)
        
        return analysis_result
    
    async def _analyze_test_results(self) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""
        test_results_dir = self.project_root / "test-results"
        
        if not test_results_dir.exists():
            return {"status": "no_data", "message": "í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ìµœê·¼ í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì¼ë“¤ ë¶„ì„
        recent_results = []
        for result_file in test_results_dir.glob("**/comprehensive_report.json"):
            if result_file.stat().st_mtime > time.time() - 7 * 24 * 3600:  # ìµœê·¼ 7ì¼
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        recent_results.append(data)
                except Exception as e:
                    logger.warning(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {result_file} - {e}")
        
        if not recent_results:
            return {"status": "no_recent_data", "message": "ìµœê·¼ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
        latest_result = recent_results[-1]
        
        analysis = {
            "status": "analyzed",
            "latest_test": latest_result.get("execution_summary", {}),
            "trends": self._analyze_test_trends(recent_results),
            "issues": []
        }
        
        # ë¬¸ì œì  ì‹ë³„
        exec_summary = latest_result.get("execution_summary", {})
        
        # ì„±ê³µë¥  í™•ì¸
        try:
            success_rate = float(exec_summary.get("success_rate", "0%").replace("%", "")) / 100
            if success_rate < 0.95:
                analysis["issues"].append({
                    "type": "low_success_rate",
                    "severity": "high",
                    "value": success_rate,
                    "threshold": 0.95,
                    "description": f"í…ŒìŠ¤íŠ¸ ì„±ê³µë¥ ì´ {success_rate:.1%}ë¡œ ëª©í‘œì¹˜ 95% ë¯¸ë‹¬"
                })
        except ValueError:
            pass
        
        # ì‹¤í–‰ ì‹œê°„ í™•ì¸
        total_duration = exec_summary.get("total_duration", 0)
        if total_duration > 3600:  # 1ì‹œê°„ ì´ˆê³¼
            analysis["issues"].append({
                "type": "slow_test_execution",
                "severity": "medium",
                "value": total_duration,
                "threshold": 3600,
                "description": f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œê°„ì´ {total_duration/60:.1f}ë¶„ìœ¼ë¡œ ê³¼ë„í•¨"
            })
        
        return analysis
    
    async def _analyze_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„"""
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°ì—ì„œ ìµœê·¼ ì„±ëŠ¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        metrics_summary = self.metrics_collector.get_metrics_summary()
        
        analysis = {
            "status": "analyzed",
            "current_metrics": metrics_summary,
            "issues": []
        }
        
        # ë©”íŠ¸ë¦­ ê¸°ë°˜ ë¬¸ì œì  ì‹ë³„ (ì‹¤ì œ ë©”íŠ¸ë¦­ì´ ìˆì„ ë•Œ êµ¬í˜„)
        # í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ë¶„ì„
        simulated_metrics = {
            "avg_response_time": 3.2,  # ì´ˆ
            "p95_response_time": 8.5,  # ì´ˆ
            "error_rate": 0.08,        # 8%
            "memory_usage": 0.85,      # 85%
            "cpu_usage": 0.75          # 75%
        }
        
        # ì‘ë‹µ ì‹œê°„ í™•ì¸
        if simulated_metrics["avg_response_time"] > self.analysis_config["response_time_threshold"]:
            analysis["issues"].append({
                "type": "slow_response_time",
                "severity": "medium",
                "value": simulated_metrics["avg_response_time"],
                "threshold": self.analysis_config["response_time_threshold"],
                "description": f"í‰ê·  ì‘ë‹µì‹œê°„ {simulated_metrics['avg_response_time']:.1f}ì´ˆê°€ ëª©í‘œì¹˜ ì´ˆê³¼"
            })
        
        # ì˜¤ë¥˜ìœ¨ í™•ì¸
        if simulated_metrics["error_rate"] > self.analysis_config["error_rate_threshold"]:
            analysis["issues"].append({
                "type": "high_error_rate",
                "severity": "high",
                "value": simulated_metrics["error_rate"],
                "threshold": self.analysis_config["error_rate_threshold"],
                "description": f"ì˜¤ë¥˜ìœ¨ {simulated_metrics['error_rate']:.1%}ê°€ ëª©í‘œì¹˜ ì´ˆê³¼"
            })
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        if simulated_metrics["memory_usage"] > self.analysis_config["memory_usage_threshold"]:
            analysis["issues"].append({
                "type": "high_memory_usage",
                "severity": "medium",
                "value": simulated_metrics["memory_usage"],
                "threshold": self.analysis_config["memory_usage_threshold"],
                "description": f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  {simulated_metrics['memory_usage']:.1%}ê°€ ì„ê³„ê°’ ì´ˆê³¼"
            })
        
        return analysis
    
    async def _analyze_user_activity(self) -> Dict[str, Any]:
        """ì‚¬ìš©ì í™œë™ ë¶„ì„"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ë¡œê·¸ì—ì„œ ì‚¬ìš©ì í™œë™ ë°ì´í„° ìˆ˜ì§‘
        # í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ë¶„ì„
        
        simulated_activity = {
            "daily_active_users": 150,
            "session_duration_avg": 25.5,  # ë¶„
            "bounce_rate": 0.35,           # 35%
            "conversion_rate": 0.12,       # 12%
            "user_satisfaction": 0.82      # 82%
        }
        
        analysis = {
            "status": "analyzed", 
            "metrics": simulated_activity,
            "issues": []
        }
        
        # ì‚¬ìš©ì ë§Œì¡±ë„ í™•ì¸
        if simulated_activity["user_satisfaction"] < self.analysis_config["user_satisfaction_threshold"]:
            analysis["issues"].append({
                "type": "low_user_satisfaction",
                "severity": "high",
                "value": simulated_activity["user_satisfaction"],
                "threshold": self.analysis_config["user_satisfaction_threshold"],
                "description": f"ì‚¬ìš©ì ë§Œì¡±ë„ {simulated_activity['user_satisfaction']:.1%}ê°€ ëª©í‘œì¹˜ ë¯¸ë‹¬"
            })
        
        # ì´íƒˆë¥  í™•ì¸
        if simulated_activity["bounce_rate"] > 0.4:
            analysis["issues"].append({
                "type": "high_bounce_rate",
                "severity": "medium",
                "value": simulated_activity["bounce_rate"],
                "threshold": 0.4,
                "description": f"ì´íƒˆë¥  {simulated_activity['bounce_rate']:.1%}ì´ ê³¼ë„í•¨"
            })
        
        return analysis
    
    async def _analyze_system_stability(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì•ˆì •ì„± ë¶„ì„"""
        # ì‹œìŠ¤í…œ ê°€ë™ì‹œê°„, ì˜¤ë¥˜ ë¡œê·¸, í¬ë˜ì‹œ ë“± ë¶„ì„
        
        analysis = {
            "status": "analyzed",
            "uptime_days": 15.5,
            "crash_count": 2,
            "critical_errors": 5,
            "issues": []
        }
        
        # í¬ë˜ì‹œ ë¹ˆë„ í™•ì¸
        if analysis["crash_count"] > 1:
            analysis["issues"].append({
                "type": "frequent_crashes",
                "severity": "critical",
                "value": analysis["crash_count"],
                "threshold": 1,
                "description": f"ìµœê·¼ {analysis['crash_count']}íšŒ í¬ë˜ì‹œ ë°œìƒ"
            })
        
        # ì‹¬ê°í•œ ì˜¤ë¥˜ í™•ì¸
        if analysis["critical_errors"] > 3:
            analysis["issues"].append({
                "type": "critical_errors",
                "severity": "high",
                "value": analysis["critical_errors"],
                "threshold": 3,
                "description": f"ì‹¬ê°í•œ ì˜¤ë¥˜ {analysis['critical_errors']}ê±´ ë°œìƒ"
            })
        
        return analysis
    
    async def _analyze_business_metrics(self) -> Dict[str, Any]:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ë¶„ì„"""
        # í”„ë¡œì íŠ¸ ì™„ë£Œìœ¨, ì‚¬ìš©ì ì¦ê°€ìœ¨, ìˆ˜ìµ ë“± ë¶„ì„
        
        simulated_business = {
            "project_completion_rate": 0.78,  # 78%
            "user_growth_rate": 0.15,         # 15% ì›”ê°„ ì¦ê°€
            "revenue_per_user": 25.50,        # ë‹¬ëŸ¬
            "churn_rate": 0.08                # 8% ì›”ê°„ ì´íƒˆ
        }
        
        analysis = {
            "status": "analyzed",
            "metrics": simulated_business,
            "issues": []
        }
        
        # í”„ë¡œì íŠ¸ ì™„ë£Œìœ¨ í™•ì¸
        if simulated_business["project_completion_rate"] < 0.8:
            analysis["issues"].append({
                "type": "low_project_completion",
                "severity": "medium",
                "value": simulated_business["project_completion_rate"],
                "threshold": 0.8,
                "description": f"í”„ë¡œì íŠ¸ ì™„ë£Œìœ¨ {simulated_business['project_completion_rate']:.1%}ê°€ ëª©í‘œì¹˜ ë¯¸ë‹¬"
            })
        
        # ì´íƒˆë¥  í™•ì¸
        if simulated_business["churn_rate"] > 0.1:
            analysis["issues"].append({
                "type": "high_churn_rate",
                "severity": "high",
                "value": simulated_business["churn_rate"],
                "threshold": 0.1,
                "description": f"ì‚¬ìš©ì ì´íƒˆë¥  {simulated_business['churn_rate']:.1%}ì´ ê³¼ë„í•¨"
            })
        
        return analysis
    
    def _analyze_test_trends(self, test_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŠ¸ë Œë“œ ë¶„ì„"""
        if len(test_results) < 2:
            return {"status": "insufficient_data"}
        
        # ì„±ê³µë¥  íŠ¸ë Œë“œ
        success_rates = []
        for result in test_results:
            try:
                rate_str = result.get("execution_summary", {}).get("success_rate", "0%")
                rate = float(rate_str.replace("%", "")) / 100
                success_rates.append(rate)
            except ValueError:
                continue
        
        trends = {
            "success_rate_trend": "stable",
            "performance_trend": "stable"
        }
        
        if len(success_rates) >= 2:
            recent_avg = np.mean(success_rates[-3:]) if len(success_rates) >= 3 else success_rates[-1]
            older_avg = np.mean(success_rates[:-3]) if len(success_rates) >= 6 else success_rates[0]
            
            if recent_avg < older_avg - 0.05:  # 5% ì´ìƒ í•˜ë½
                trends["success_rate_trend"] = "declining"
            elif recent_avg > older_avg + 0.05:  # 5% ì´ìƒ ìƒìŠ¹
                trends["success_rate_trend"] = "improving"
        
        return trends
    
    def _calculate_overall_health_score(self, analysis_result: Dict[str, Any]) -> float:
        """ì „ì²´ ì‹œìŠ¤í…œ ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°"""
        scores = []
        weights = []
        
        # ê° ë°ì´í„° ì†ŒìŠ¤ë³„ ì ìˆ˜ ê³„ì‚°
        for source_name, source_data in analysis_result["data_sources"].items():
            if source_data.get("status") == "analyzed":
                issue_count = len(source_data.get("issues", []))
                critical_issues = sum(1 for issue in source_data.get("issues", []) if issue.get("severity") == "critical")
                high_issues = sum(1 for issue in source_data.get("issues", []) if issue.get("severity") == "high")
                
                # ì ìˆ˜ ê³„ì‚° (1.0ì´ ìµœê³ , 0.0ì´ ìµœì €)
                score = 1.0 - (critical_issues * 0.4 + high_issues * 0.2 + (issue_count - critical_issues - high_issues) * 0.1)
                score = max(0.0, score)
                
                scores.append(score)
                weights.append(1.0)
        
        if not scores:
            return 0.5  # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¤‘ê°„ ì ìˆ˜
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_score = np.average(scores, weights=weights)
        return round(weighted_score, 3)
    
    async def _identify_improvement_opportunities(self, analysis_result: Dict[str, Any]) -> List[ImprovementItem]:
        """ê°œì„  ê¸°íšŒ ì‹ë³„"""
        opportunities = []
        
        # ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ì˜ ì´ìŠˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œì„  í•­ëª© ìƒì„±
        for source_name, source_data in analysis_result["data_sources"].items():
            if source_data.get("status") != "analyzed":
                continue
            
            for issue in source_data.get("issues", []):
                improvement_item = self._create_improvement_item_from_issue(source_name, issue)
                if improvement_item:
                    opportunities.append(improvement_item)
        
        # ì¶”ê°€ ê°œì„  ê¸°íšŒ ì‹ë³„ (íŒ¨í„´ ê¸°ë°˜)
        pattern_opportunities = await self._identify_pattern_based_opportunities(analysis_result)
        opportunities.extend(pattern_opportunities)
        
        return opportunities
    
    def _create_improvement_item_from_issue(self, source: str, issue: Dict[str, Any]) -> Optional[ImprovementItem]:
        """ì´ìŠˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œì„  í•­ëª© ìƒì„±"""
        issue_type = issue.get("type", "unknown")
        severity = issue.get("severity", "medium")
        
        # ìš°ì„ ìˆœìœ„ ë§¤í•‘
        priority_map = {
            "critical": ImprovementPriority.CRITICAL,
            "high": ImprovementPriority.HIGH,
            "medium": ImprovementPriority.MEDIUM,
            "low": ImprovementPriority.LOW
        }
        
        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        category_map = {
            "slow_response_time": ImprovementCategory.PERFORMANCE,
            "high_error_rate": ImprovementCategory.RELIABILITY,
            "high_memory_usage": ImprovementCategory.PERFORMANCE,
            "low_success_rate": ImprovementCategory.RELIABILITY,
            "slow_test_execution": ImprovementCategory.PERFORMANCE,
            "low_user_satisfaction": ImprovementCategory.USABILITY,
            "high_bounce_rate": ImprovementCategory.USABILITY,
            "frequent_crashes": ImprovementCategory.RELIABILITY,
            "critical_errors": ImprovementCategory.RELIABILITY,
            "low_project_completion": ImprovementCategory.USABILITY,
            "high_churn_rate": ImprovementCategory.USABILITY
        }
        
        if issue_type not in category_map:
            return None
        
        # ì˜í–¥ë„ ë° ë…¸ë ¥ë„ ì¶”ì •
        impact_scores = {
            "critical": 0.9,
            "high": 0.7,
            "medium": 0.5,
            "low": 0.3
        }
        
        effort_scores = {
            "slow_response_time": 0.6,
            "high_error_rate": 0.7,
            "high_memory_usage": 0.4,
            "low_success_rate": 0.8,
            "slow_test_execution": 0.3,
            "low_user_satisfaction": 0.8,
            "high_bounce_rate": 0.6,
            "frequent_crashes": 0.9,
            "critical_errors": 0.7,
            "low_project_completion": 0.7,
            "high_churn_rate": 0.8
        }
        
        improvement_item = ImprovementItem(
            id=f"{source}_{issue_type}_{int(time.time())}",
            title=f"{issue_type.replace('_', ' ').title()} ê°œì„ ",
            description=issue.get("description", ""),
            category=category_map[issue_type],
            priority=priority_map[severity],
            impact_score=impact_scores.get(severity, 0.5),
            effort_score=effort_scores.get(issue_type, 0.5),
            current_metrics={"value": issue.get("value", 0)},
            target_metrics={"value": issue.get("threshold", 0)},
            source_data={"source": source, "issue": issue}
        )
        
        return improvement_item
    
    async def _identify_pattern_based_opportunities(self, analysis_result: Dict[str, Any]) -> List[ImprovementItem]:
        """íŒ¨í„´ ê¸°ë°˜ ê°œì„  ê¸°íšŒ ì‹ë³„"""
        opportunities = []
        
        # ì „ì²´ ê±´ê°•ë„ê°€ ë‚®ì€ ê²½ìš° ì¢…í•© ê°œì„  ì œì•ˆ
        health_score = analysis_result.get("overall_health_score", 0.5)
        if health_score < 0.7:
            opportunities.append(ImprovementItem(
                id=f"comprehensive_improvement_{int(time.time())}",
                title="ì¢…í•©ì  ì‹œìŠ¤í…œ ê°œì„ ",
                description=f"ì „ì²´ ì‹œìŠ¤í…œ ê±´ê°•ë„({health_score:.1%})ê°€ ë‚®ì•„ ì¢…í•©ì ì¸ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤",
                category=ImprovementCategory.RELIABILITY,
                priority=ImprovementPriority.HIGH,
                impact_score=0.8,
                effort_score=0.9,
                current_metrics={"health_score": health_score},
                target_metrics={"health_score": 0.85}
            ))
        
        # ì—¬ëŸ¬ ì„±ëŠ¥ ì´ìŠˆê°€ ìˆëŠ” ê²½ìš° ì„±ëŠ¥ ìµœì í™” ì œì•ˆ
        perf_issues = 0
        for source_data in analysis_result["data_sources"].values():
            for issue in source_data.get("issues", []):
                if issue.get("type", "").startswith(("slow_", "high_memory", "high_error")):
                    perf_issues += 1
        
        if perf_issues >= 2:
            opportunities.append(ImprovementItem(
                id=f"performance_optimization_{int(time.time())}",
                title="ì„±ëŠ¥ ìµœì í™” í”„ë¡œê·¸ë¨",
                description=f"{perf_issues}ê°œì˜ ì„±ëŠ¥ ê´€ë ¨ ì´ìŠˆ ë°œê²¬. í†µí•© ì„±ëŠ¥ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤",
                category=ImprovementCategory.PERFORMANCE,
                priority=ImprovementPriority.HIGH,
                impact_score=0.7,
                effort_score=0.6,
                current_metrics={"performance_issues": perf_issues},
                target_metrics={"performance_issues": 0}
            ))
        
        return opportunities
    
    async def _prioritize_improvements(self, opportunities: List[ImprovementItem]) -> List[ImprovementItem]:
        """ê°œì„  í•­ëª© ìš°ì„ ìˆœìœ„ ê²°ì •"""
        # ROI ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        prioritized = sorted(opportunities, key=lambda x: (
            x.priority.value == "critical",  # Critical ìš°ì„ 
            x.roi_score,                     # ROI ì ìˆ˜
            x.impact_score                   # ì˜í–¥ë„
        ), reverse=True)
        
        return prioritized
    
    async def _execute_automatic_improvements(self, improvements: List[ImprovementItem]) -> Dict[str, Any]:
        """ìë™ ê°œì„  ì‹¤í–‰"""
        auto_results = {
            "executed": [],
            "skipped": [],
            "failed": []
        }
        
        for improvement in improvements[:5]:  # ìƒìœ„ 5ê°œë§Œ ìë™ ì‹¤í–‰ ê³ ë ¤
            if await self._can_auto_execute(improvement):
                try:
                    result = await self._execute_single_improvement(improvement)
                    auto_results["executed"].append({
                        "improvement_id": improvement.id,
                        "title": improvement.title,
                        "result": result
                    })
                    improvement.status = "completed"
                    improvement.actual_completion = datetime.now()
                except Exception as e:
                    auto_results["failed"].append({
                        "improvement_id": improvement.id,
                        "title": improvement.title,
                        "error": str(e)
                    })
                    improvement.status = "failed"
            else:
                auto_results["skipped"].append({
                    "improvement_id": improvement.id,
                    "title": improvement.title,
                    "reason": "ìˆ˜ë™ ì‹¤í–‰ í•„ìš”"
                })
        
        return auto_results
    
    async def _can_auto_execute(self, improvement: ImprovementItem) -> bool:
        """ìë™ ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨"""
        # ë‚®ì€ ë¦¬ìŠ¤í¬, ë†’ì€ í™•ì‹ ë„ì˜ ê°œì„ ë§Œ ìë™ ì‹¤í–‰
        auto_executable_types = [
            "slow_test_execution",  # í…ŒìŠ¤íŠ¸ ìµœì í™”
            "high_memory_usage"     # ë©”ëª¨ë¦¬ ì •ë¦¬
        ]
        
        issue_type = improvement.source_data.get("issue", {}).get("type", "")
        return (
            issue_type in auto_executable_types and
            improvement.effort_score < 0.4 and  # ë‚®ì€ ë…¸ë ¥
            improvement.impact_score > 0.5       # ë†’ì€ ì˜í–¥
        )
    
    async def _execute_single_improvement(self, improvement: ImprovementItem) -> Dict[str, Any]:
        """ë‹¨ì¼ ê°œì„  í•­ëª© ì‹¤í–‰"""
        issue_type = improvement.source_data.get("issue", {}).get("type", "")
        
        if issue_type == "slow_test_execution":
            return await self._optimize_test_execution()
        elif issue_type == "high_memory_usage":
            return await self._optimize_memory_usage()
        else:
            raise NotImplementedError(f"ìë™ ì‹¤í–‰ ë¯¸êµ¬í˜„: {issue_type}")
    
    async def _optimize_test_execution(self) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìµœì í™”"""
        # í…ŒìŠ¤íŠ¸ ë³‘ë ¬í™”, ìºì‹± ë“±ì˜ ìµœì í™” ìˆ˜í–‰
        logger.info("í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìµœì í™” ìˆ˜í–‰ ì¤‘...")
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ìµœì í™”
        await asyncio.sleep(2)  # ì‹¤ì œ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
        
        return {
            "action": "test_optimization",
            "changes": [
                "í…ŒìŠ¤íŠ¸ ë³‘ë ¬í™” í™œì„±í™”",
                "í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìºì‹± êµ¬í˜„",
                "ë¶ˆí•„ìš”í•œ í…ŒìŠ¤íŠ¸ ì œê±°"
            ],
            "estimated_improvement": "30% ì†ë„ í–¥ìƒ"
        }
    
    async def _optimize_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ìˆ˜ì •, ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™” ë“±
        logger.info("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ìˆ˜í–‰ ì¤‘...")
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ìµœì í™”
        await asyncio.sleep(3)  # ì‹¤ì œ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
        
        return {
            "action": "memory_optimization",
            "changes": [
                "ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ìˆ˜ì •",
                "ê°ì²´ í’€ë§ êµ¬í˜„",
                "ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ íŠœë‹"
            ],
            "estimated_improvement": "20% ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ"
        }
    
    async def _generate_manual_improvement_plans(self, improvements: List[ImprovementItem]) -> List[Dict[str, Any]]:
        """ìˆ˜ë™ ê°œì„  ê³„íš ìƒì„±"""
        manual_plans = []
        
        for improvement in improvements:
            if improvement.status in ["identified", "planned"]:
                plan = await self._create_improvement_plan(improvement)
                manual_plans.append(plan)
        
        return manual_plans
    
    async def _create_improvement_plan(self, improvement: ImprovementItem) -> Dict[str, Any]:
        """ê°œì„  ê³„íš ìƒì„±"""
        plan = {
            "improvement_id": improvement.id,
            "title": improvement.title,
            "category": improvement.category.value,
            "priority": improvement.priority.value,
            "roi_score": improvement.roi_score,
            "description": improvement.description,
            "estimated_effort": self._estimate_effort_days(improvement.effort_score),
            "success_criteria": self._define_success_criteria(improvement),
            "implementation_steps": self._generate_implementation_steps(improvement),
            "risks": self._identify_risks(improvement),
            "resources_needed": self._identify_resources(improvement)
        }
        
        return plan
    
    def _estimate_effort_days(self, effort_score: float) -> int:
        """ë…¸ë ¥ ì ìˆ˜ë¥¼ ì¼ìˆ˜ë¡œ ë³€í™˜"""
        # 0.0 - 1.0 ì ìˆ˜ë¥¼ 1-30ì¼ë¡œ ë§¤í•‘
        return max(1, int(effort_score * 30))
    
    def _define_success_criteria(self, improvement: ImprovementItem) -> List[str]:
        """ì„±ê³µ ê¸°ì¤€ ì •ì˜"""
        criteria = []
        
        for metric_name, target_value in improvement.target_metrics.items():
            current_value = improvement.current_metrics.get(metric_name, 0)
            criteria.append(f"{metric_name}ë¥¼ {current_value}ì—ì„œ {target_value}ë¡œ ê°œì„ ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì¶”ê°€ ê¸°ì¤€
        if improvement.category == ImprovementCategory.PERFORMANCE:
            criteria.append("ì„±ëŠ¥ ê°œì„  í›„ 1ì£¼ì¼ê°„ ì•ˆì •ì  ìš´ì˜")
        elif improvement.category == ImprovementCategory.RELIABILITY:
            criteria.append("ì˜¤ë¥˜ìœ¨ ê°œì„  í›„ ì§€ì†ì  ëª¨ë‹ˆí„°ë§ í™•ì¸")
        
        return criteria
    
    def _generate_implementation_steps(self, improvement: ImprovementItem) -> List[str]:
        """êµ¬í˜„ ë‹¨ê³„ ìƒì„±"""
        issue_type = improvement.source_data.get("issue", {}).get("type", "")
        
        step_templates = {
            "slow_response_time": [
                "ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰",
                "ë³‘ëª©ì§€ì  ì‹ë³„ ë° ë¶„ì„",
                "ìµœì í™” ì „ëµ ìˆ˜ë¦½",
                "ë‹¨ê³„ì  ìµœì í™” êµ¬í˜„",
                "ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦"
            ],
            "high_error_rate": [
                "ì˜¤ë¥˜ ë¡œê·¸ ìƒì„¸ ë¶„ì„",
                "ê·¼ë³¸ ì›ì¸ ì‹ë³„",
                "ì˜¤ë¥˜ ì²˜ë¦¬ ë¡œì§ ê°œì„ ",
                "ì˜ˆì™¸ ìƒí™© í…ŒìŠ¤íŠ¸",
                "ëª¨ë‹ˆí„°ë§ ê°•í™”"
            ],
            "low_user_satisfaction": [
                "ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘",
                "UX/UI ê°œì„ ì  ì‹ë³„",
                "í”„ë¡œí† íƒ€ì… ê°œë°œ",
                "ì‚¬ìš©ì í…ŒìŠ¤íŠ¸",
                "ì ì§„ì  ê°œì„  ë°°í¬"
            ]
        }
        
        return step_templates.get(issue_type, [
            "ë¬¸ì œ ìƒí™© ìƒì„¸ ë¶„ì„",
            "í•´ê²° ë°©ì•ˆ ì—°êµ¬",
            "êµ¬í˜„ ê³„íš ìˆ˜ë¦½",
            "ë‹¨ê³„ì  êµ¬í˜„",
            "í…ŒìŠ¤íŠ¸ ë° ê²€ì¦"
        ])
    
    def _identify_risks(self, improvement: ImprovementItem) -> List[str]:
        """ìœ„í—˜ ìš”ì†Œ ì‹ë³„"""
        risks = []
        
        if improvement.effort_score > 0.7:
            risks.append("ë†’ì€ êµ¬í˜„ ë³µì¡ë„ë¡œ ì¸í•œ ì¼ì • ì§€ì—° ìœ„í—˜")
        
        if improvement.category == ImprovementCategory.PERFORMANCE:
            risks.append("ì„±ëŠ¥ ìµœì í™”ë¡œ ì¸í•œ ê¸°ëŠ¥ ì•ˆì •ì„± ì˜í–¥")
        
        if improvement.priority == ImprovementPriority.CRITICAL:
            risks.append("ê¸´ê¸‰ ê°œì„ ìœ¼ë¡œ ì¸í•œ ì¶©ë¶„í•˜ì§€ ì•Šì€ í…ŒìŠ¤íŠ¸")
        
        return risks
    
    def _identify_resources(self, improvement: ImprovementItem) -> List[str]:
        """í•„ìš” ë¦¬ì†ŒìŠ¤ ì‹ë³„"""
        resources = []
        
        if improvement.category == ImprovementCategory.PERFORMANCE:
            resources.extend(["ë°±ì—”ë“œ ê°œë°œì", "ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë„êµ¬"])
        
        if improvement.category == ImprovementCategory.USABILITY:
            resources.extend(["UX/UI ë””ìì´ë„ˆ", "í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì"])
        
        if improvement.effort_score > 0.6:
            resources.append("í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €")
        
        return resources
    
    async def _generate_improvement_report(self, cycle_results: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œì„  ë³´ê³ ì„œ ìƒì„±"""
        report = {
            "executive_summary": self._generate_executive_summary(cycle_results),
            "detailed_findings": cycle_results["stages"]["data_analysis"],
            "improvement_opportunities": len(cycle_results["stages"]["opportunity_identification"]),
            "automatic_improvements": cycle_results["stages"]["automatic_improvements"],
            "manual_improvement_plans": cycle_results["stages"]["manual_improvement_plans"],
            "recommendations": self._generate_recommendations(cycle_results),
            "next_cycle_date": (datetime.now() + timedelta(weeks=2)).isoformat()
        }
        
        return report
    
    def _generate_executive_summary(self, cycle_results: Dict[str, Any]) -> str:
        """ê²½ì˜ì§„ ìš”ì•½ ìƒì„±"""
        analysis = cycle_results["stages"]["data_analysis"]
        health_score = analysis.get("overall_health_score", 0.5)
        
        auto_improvements = cycle_results["stages"]["automatic_improvements"]
        executed_count = len(auto_improvements["executed"])
        
        opportunities = cycle_results["stages"]["opportunity_identification"]
        high_priority = sum(1 for item in opportunities if item.priority == ImprovementPriority.HIGH)
        
        summary = f"""
VIBA AI ì‹œìŠ¤í…œ ê±´ê°•ë„: {health_score:.1%}

ì´ë²ˆ ê°œì„  ì‚¬ì´í´ì—ì„œ {len(opportunities)}ê°œì˜ ê°œì„  ê¸°íšŒë¥¼ ì‹ë³„í–ˆìŠµë‹ˆë‹¤.
ê·¸ ì¤‘ {executed_count}ê°œëŠ” ìë™ìœ¼ë¡œ ê°œì„ ë˜ì—ˆê³ , {high_priority}ê°œì˜ ê³ ìš°ì„ ìˆœìœ„ í•­ëª©ì´ 
ìˆ˜ë™ ê°œì„  ê³„íšì— í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.

ì£¼ìš” ê°œì„  ì˜ì—­:
- ì„±ëŠ¥ ìµœì í™”
- ì‹œìŠ¤í…œ ì•ˆì •ì„±
- ì‚¬ìš©ì ê²½í—˜ ê°œì„ 
        """.strip()
        
        return summary
    
    def _generate_recommendations(self, cycle_results: Dict[str, Any]) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        analysis = cycle_results["stages"]["data_analysis"]
        health_score = analysis.get("overall_health_score", 0.5)
        
        if health_score < 0.7:
            recommendations.append("ì‹œìŠ¤í…œ ê±´ê°•ë„ê°€ ë‚®ì•„ ê¸´ê¸‰ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        auto_improvements = cycle_results["stages"]["automatic_improvements"]
        if len(auto_improvements["failed"]) > 0:
            recommendations.append("ìë™ ê°œì„  ì‹¤íŒ¨ í•­ëª©ë“¤ì„ ìˆ˜ë™ìœ¼ë¡œ ê²€í† í•˜ì„¸ìš”")
        
        manual_plans = cycle_results["stages"]["manual_improvement_plans"]
        if len(manual_plans) > 5:
            recommendations.append("ê°œì„  í•­ëª©ì´ ë§ì•„ ìš°ì„ ìˆœìœ„ë¥¼ ì¬ê²€í† í•˜ì„¸ìš”")
        
        return recommendations
    
    async def _save_cycle_results(self, cycle_results: Dict[str, Any]):
        """ì‚¬ì´í´ ê²°ê³¼ ì €ì¥"""
        cycle_file = self.data_dir / f"improvement_cycle_{cycle_results['cycle_id']}.json"
        
        try:
            with open(cycle_file, 'w', encoding='utf-8') as f:
                json.dump(cycle_results, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"ê°œì„  ì‚¬ì´í´ ê²°ê³¼ ì €ì¥: {cycle_file}")
            
        except Exception as e:
            logger.error(f"ê°œì„  ì‚¬ì´í´ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸ”„ VIBA AI ì§€ì†ì  ê°œì„  ì›Œí¬í”Œë¡œìš° ì‹œì‘")
    
    # ì§€ì†ì  ê°œì„  ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    improvement_system = VIBAContinuousImprovement()
    
    # ê°œì„  ì‚¬ì´í´ ì‹¤í–‰
    results = await improvement_system.run_improvement_cycle()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ”„ VIBA AI ì§€ì†ì  ê°œì„  ê²°ê³¼")
    print("="*60)
    
    print(f"ì‚¬ì´í´ ID: {results['cycle_id']}")
    print(f"ì‹¤í–‰ ì‹œê°„: {results.get('duration', 0):.2f}ì´ˆ")
    print(f"ìƒíƒœ: {results['status']}")
    
    if results['status'] == 'completed':
        final_report = results.get('final_report', {})
        print(f"\nì‹œìŠ¤í…œ ê±´ê°•ë„: {results['stages']['data_analysis'].get('overall_health_score', 0):.1%}")
        print(f"ì‹ë³„ëœ ê°œì„  ê¸°íšŒ: {final_report.get('improvement_opportunities', 0)}ê°œ")
        print(f"ìë™ ê°œì„  ì‹¤í–‰: {len(final_report.get('automatic_improvements', {}).get('executed', []))}ê°œ")
        print(f"ìˆ˜ë™ ê°œì„  ê³„íš: {len(final_report.get('manual_improvement_plans', []))}ê°œ")
        
        print("\nê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(final_report.get('recommendations', []), 1):
            print(f"  {i}. {rec}")
        
        print(f"\në‹¤ìŒ ì‚¬ì´í´: {final_report.get('next_cycle_date', 'TBD')}")
    
    print("\n" + "="*60)


if __name__ == "__main__":
    asyncio.run(main())