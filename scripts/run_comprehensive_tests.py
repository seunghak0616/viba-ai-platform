#!/usr/bin/env python3
"""
VIBA AI ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
==============================

ì „ì²´ AI ì‹œìŠ¤í…œì˜ ë‹¨ìœ„, í†µí•©, ì„±ëŠ¥, E2E í…ŒìŠ¤íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³ 
ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

@version 1.0
@author VIBA AI Team
@date 2025.07.06
"""

import os
import sys
import subprocess
import json
import time
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from nlp_engine.src.utils.logger import setup_logger

logger = setup_logger(__name__)


class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    
    def __init__(self, test_name: str, test_type: str):
        self.test_name = test_name
        self.test_type = test_type
        self.start_time = time.time()
        self.end_time: Optional[float] = None
        self.duration: Optional[float] = None
        self.status = "running"
        self.exit_code: Optional[int] = None
        self.stdout = ""
        self.stderr = ""
        self.metrics: Dict[str, Any] = {}
        
    def complete(self, exit_code: int, stdout: str = "", stderr: str = ""):
        """í…ŒìŠ¤íŠ¸ ì™„ë£Œ ì²˜ë¦¬"""
        self.end_time = time.time()
        self.duration = self.end_time - self.start_time
        self.exit_code = exit_code
        self.stdout = stdout
        self.stderr = stderr
        self.status = "passed" if exit_code == 0 else "failed"
        
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'test_name': self.test_name,
            'test_type': self.test_type,
            'status': self.status,
            'duration': self.duration,
            'exit_code': self.exit_code,
            'start_time': self.start_time,
            'end_time': self.end_time,
            'stdout': self.stdout,
            'stderr': self.stderr,
            'metrics': self.metrics
        }


class VIBATestRunner:
    """VIBA AI ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.project_root = project_root
        self.test_results: List[TestResult] = []
        self.start_time = time.time()
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        self.results_dir = self.project_root / "test-results"
        self.results_dir.mkdir(exist_ok=True)
        
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.session_dir = self.results_dir / f"session_{self.timestamp}"
        self.session_dir.mkdir(exist_ok=True)
        
        logger.info(f"í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ì‹œì‘: {self.session_dir}")
    
    def run_command(self, command: List[str], cwd: Optional[Path] = None, env: Optional[Dict[str, str]] = None) -> TestResult:
        """ëª…ë ¹ì–´ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘"""
        test_name = " ".join(command)
        result = TestResult(test_name, "command")
        
        logger.info(f"ğŸ”„ ì‹¤í–‰ ì¤‘: {test_name}")
        
        try:
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            test_env = os.environ.copy()
            if env:
                test_env.update(env)
            
            # ëª…ë ¹ì–´ ì‹¤í–‰
            process = subprocess.run(
                command,
                cwd=cwd or self.project_root,
                capture_output=True,
                text=True,
                env=test_env,
                timeout=self.config.get('test_timeout', 600)  # 10ë¶„ ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ
            )
            
            result.complete(process.returncode, process.stdout, process.stderr)
            
            if result.status == "passed":
                logger.info(f"âœ… ì„±ê³µ: {test_name} ({result.duration:.2f}ì´ˆ)")
            else:
                logger.error(f"âŒ ì‹¤íŒ¨: {test_name} ({result.duration:.2f}ì´ˆ)")
                logger.error(f"Error output: {process.stderr}")
                
        except subprocess.TimeoutExpired:
            result.complete(124, "", "Test timed out")
            logger.error(f"â° íƒ€ì„ì•„ì›ƒ: {test_name}")
        except Exception as e:
            result.complete(1, "", str(e))
            logger.error(f"ğŸ’¥ ì˜ˆì™¸ ë°œìƒ: {test_name} - {e}")
        
        self.test_results.append(result)
        return result
    
    def run_unit_tests(self) -> Dict[str, TestResult]:
        """ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ§ª ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘...")
        
        unit_tests = {}
        
        # Python ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
        python_tests = [
            "nlp_engine_tests.py",
            "theory_application_tests.py", 
            "bim_generation_tests.py",
            "performance_analysis_tests.py"
        ]
        
        for test_file in python_tests:
            test_name = f"unit_{test_file.replace('.py', '')}"
            result = self.run_command([
                "python", "-m", "pytest", 
                f"tests/unit/{test_file}",
                "-v", "--tb=short",
                f"--junitxml={self.session_dir}/junit_{test_name}.xml",
                f"--cov-report=xml:{self.session_dir}/coverage_{test_name}.xml"
            ])
            unit_tests[test_name] = result
        
        # TypeScript ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (Frontend)
        frontend_result = self.run_command([
            "npm", "run", "test:unit:frontend"
        ], cwd=self.project_root / "frontend")
        unit_tests["unit_frontend"] = frontend_result
        
        # TypeScript ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (Backend)
        backend_result = self.run_command([
            "npm", "run", "test:unit:backend"
        ], cwd=self.project_root / "backend")
        unit_tests["unit_backend"] = backend_result
        
        return unit_tests
    
    def run_integration_tests(self) -> Dict[str, TestResult]:
        """í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ”— í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘...")
        
        integration_tests = {}
        
        # ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í†µí•© í…ŒìŠ¤íŠ¸
        multi_agent_result = self.run_command([
            "python", "-m", "pytest",
            "tests/integration/multi_agent_integration_tests.py",
            "-v", "--tb=short",
            f"--junitxml={self.session_dir}/junit_integration_multi_agent.xml"
        ])
        integration_tests["multi_agent"] = multi_agent_result
        
        # MCP í†µí•© í…ŒìŠ¤íŠ¸
        mcp_result = self.run_command([
            "python", "-m", "pytest",
            "tests/mcp/mcp_integration_tests.py", 
            "-v", "--tb=short",
            f"--junitxml={self.session_dir}/junit_integration_mcp.xml"
        ], env={"TEST_MODE": "true"})
        integration_tests["mcp"] = mcp_result
        
        # API í†µí•© í…ŒìŠ¤íŠ¸
        api_result = self.run_command([
            "npm", "run", "test:integration:api"
        ], cwd=self.project_root / "backend")
        integration_tests["api"] = api_result
        
        return integration_tests
    
    def run_performance_tests(self) -> Dict[str, TestResult]:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘...")
        
        performance_tests = {}
        
        # AI ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        ai_performance_result = self.run_command([
            "python", "nlp-engine/tests/performance/model_performance_test.py"
        ])
        performance_tests["ai_models"] = ai_performance_result
        
        # API ë¶€í•˜ í…ŒìŠ¤íŠ¸ (K6ê°€ ì„¤ì¹˜ëœ ê²½ìš°)
        if self._check_k6_installed():
            api_load_result = self.run_command([
                "k6", "run", "tests/performance/api_load_test.js"
            ])
            performance_tests["api_load"] = api_load_result
        else:
            logger.warning("K6ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ API ë¶€í•˜ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
        
        return performance_tests
    
    def run_e2e_tests(self) -> Dict[str, TestResult]:
        """E2E í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ­ E2E í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘...")
        
        e2e_tests = {}
        
        # Playwright E2E í…ŒìŠ¤íŠ¸
        if self._check_playwright_installed():
            e2e_result = self.run_command([
                "npx", "playwright", "test", "tests/e2e/",
                "--reporter=json",
                f"--output={self.session_dir}/playwright-results/"
            ], cwd=self.project_root / "frontend")
            e2e_tests["playwright"] = e2e_result
        else:
            logger.warning("Playwrightê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ E2E í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
        
        return e2e_tests
    
    def run_security_tests(self) -> Dict[str, TestResult]:
        """ë³´ì•ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ›¡ï¸ ë³´ì•ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘...")
        
        security_tests = {}
        
        # npm audit
        npm_audit_result = self.run_command([
            "npm", "audit", "--audit-level", "high"
        ])
        security_tests["npm_audit"] = npm_audit_result
        
        # Python ë³´ì•ˆ ìŠ¤ìº” (bandit)
        bandit_result = self.run_command([
            "bandit", "-r", "nlp-engine/src/", 
            "-f", "json", "-o", f"{self.session_dir}/bandit_report.json"
        ])
        security_tests["bandit"] = bandit_result
        
        return security_tests
    
    def run_code_quality_tests(self) -> Dict[str, TestResult]:
        """ì½”ë“œ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ¯ ì½”ë“œ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘...")
        
        quality_tests = {}
        
        # Python ì½”ë“œ í’ˆì§ˆ
        python_lint_result = self.run_command([
            "npm", "run", "lint:python:check"
        ])
        quality_tests["python_lint"] = python_lint_result
        
        # TypeScript ì½”ë“œ í’ˆì§ˆ
        typescript_lint_result = self.run_command([
            "npm", "run", "lint:check"
        ])
        quality_tests["typescript_lint"] = typescript_lint_result
        
        # íƒ€ì… ì²´í¬
        type_check_result = self.run_command([
            "npm", "run", "type-check"
        ])
        quality_tests["type_check"] = type_check_result
        
        return quality_tests
    
    def _check_k6_installed(self) -> bool:
        """K6 ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸"""
        try:
            subprocess.run(["k6", "version"], capture_output=True, check=True)
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            return False
    
    def _check_playwright_installed(self) -> bool:
        """Playwright ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸"""
        try:
            result = subprocess.run(
                ["npx", "playwright", "--version"], 
                capture_output=True, 
                cwd=self.project_root / "frontend"
            )
            return result.returncode == 0
        except FileNotFoundError:
            return False
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        logger.info("ğŸ“Š ì¢…í•© í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        total_duration = time.time() - self.start_time
        
        # ê²°ê³¼ í†µê³„ ê³„ì‚°
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r.status == "passed")
        failed_tests = sum(1 for r in self.test_results if r.status == "failed")
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        # í…ŒìŠ¤íŠ¸ íƒ€ì…ë³„ ë¶„ë¥˜
        results_by_type = {}
        for result in self.test_results:
            test_type = result.test_type
            if test_type not in results_by_type:
                results_by_type[test_type] = []
            results_by_type[test_type].append(result.to_dict())
        
        # ì¢…í•© ë¦¬í¬íŠ¸
        comprehensive_report = {
            "execution_summary": {
                "timestamp": self.timestamp,
                "total_duration": total_duration,
                "total_tests": total_tests,
                "passed": passed_tests,
                "failed": failed_tests,
                "success_rate": f"{success_rate:.1f}%",
                "test_coverage": "ê³„ì‚° ì¤‘...",  # ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ê³„ì‚°
            },
            "test_results_by_type": results_by_type,
            "detailed_results": [r.to_dict() for r in self.test_results],
            "environment_info": {
                "python_version": sys.version,
                "node_version": self._get_node_version(),
                "platform": sys.platform,
                "test_session_id": self.timestamp
            },
            "recommendations": self._generate_recommendations()
        }
        
        # JSON ë¦¬í¬íŠ¸ ì €ì¥
        report_file = self.session_dir / "comprehensive_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
        
        # HTML ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_html_report(comprehensive_report)
        
        logger.info(f"ğŸ“‹ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {report_file}")
        
        return comprehensive_report
    
    def _get_node_version(self) -> str:
        """Node.js ë²„ì „ í™•ì¸"""
        try:
            result = subprocess.run(["node", "--version"], capture_output=True, text=True)
            return result.stdout.strip() if result.returncode == 0 else "Unknown"
        except FileNotFoundError:
            return "Not installed"
    
    def _generate_recommendations(self) -> List[str]:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        failed_tests = [r for r in self.test_results if r.status == "failed"]
        
        if failed_tests:
            recommendations.append(f"{len(failed_tests)}ê°œì˜ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤")
        
        slow_tests = [r for r in self.test_results if r.duration and r.duration > 30]
        if slow_tests:
            recommendations.append(f"{len(slow_tests)}ê°œì˜ ëŠë¦° í…ŒìŠ¤íŠ¸ (>30ì´ˆ) ì„±ëŠ¥ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        success_rate = len([r for r in self.test_results if r.status == "passed"]) / len(self.test_results) * 100
        if success_rate < 95:
            recommendations.append("ì „ì²´ ì„±ê³µë¥ ì´ 95% ë¯¸ë§Œì…ë‹ˆë‹¤. í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        return recommendations
    
    def _generate_html_report(self, report_data: Dict[str, Any]):
        """HTML ë¦¬í¬íŠ¸ ìƒì„±"""
        html_template = """
        <!DOCTYPE html>
        <html lang="ko">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>VIBA AI ì¢…í•© í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸</title>
            <style>
                body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }
                .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
                h1 { color: #2c3e50; text-align: center; margin-bottom: 30px; }
                .summary { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin-bottom: 30px; }
                .metric { background: #ecf0f1; padding: 20px; border-radius: 8px; text-align: center; }
                .metric h3 { margin: 0 0 10px 0; color: #34495e; }
                .metric .value { font-size: 2em; font-weight: bold; color: #2980b9; }
                .success { color: #27ae60; }
                .warning { color: #f39c12; }
                .error { color: #e74c3c; }
                .test-results { margin-top: 30px; }
                .test-group { margin-bottom: 25px; border: 1px solid #ddd; border-radius: 5px; }
                .test-group h3 { background: #34495e; color: white; margin: 0; padding: 15px; }
                .test-item { padding: 10px 15px; border-bottom: 1px solid #eee; display: flex; justify-content: space-between; align-items: center; }
                .test-item:last-child { border-bottom: none; }
                .status-badge { padding: 4px 8px; border-radius: 4px; color: white; font-size: 12px; font-weight: bold; }
                .passed { background: #27ae60; }
                .failed { background: #e74c3c; }
                .duration { color: #7f8c8d; font-size: 14px; }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸ¤– VIBA AI ì¢…í•© í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸</h1>
                
                <div class="summary">
                    <div class="metric">
                        <h3>ì´ í…ŒìŠ¤íŠ¸</h3>
                        <div class="value">{total_tests}</div>
                    </div>
                    <div class="metric">
                        <h3>ì„±ê³µ</h3>
                        <div class="value success">{passed}</div>
                    </div>
                    <div class="metric">
                        <h3>ì‹¤íŒ¨</h3>
                        <div class="value error">{failed}</div>
                    </div>
                    <div class="metric">
                        <h3>ì„±ê³µë¥ </h3>
                        <div class="value">{success_rate}</div>
                    </div>
                    <div class="metric">
                        <h3>ì‹¤í–‰ ì‹œê°„</h3>
                        <div class="value">{duration:.1f}ë¶„</div>
                    </div>
                </div>
                
                <div class="test-results">
                    {test_results_html}
                </div>
                
                <div style="margin-top: 30px; padding: 20px; background: #f8f9fa; border-radius: 5px;">
                    <h3>ê°œì„  ê¶Œì¥ì‚¬í•­</h3>
                    <ul>
                        {recommendations_html}
                    </ul>
                </div>
                
                <div style="margin-top: 20px; text-align: center; color: #7f8c8d; font-size: 14px;">
                    ìƒì„± ì‹œê°„: {timestamp} | VIBA AI Platform
                </div>
            </div>
        </body>
        </html>
        """
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ HTML ìƒì„±
        test_results_html = ""
        for test_type, results in report_data["test_results_by_type"].items():
            test_results_html += f'<div class="test-group"><h3>{test_type.upper()} í…ŒìŠ¤íŠ¸</h3>'
            for result in results:
                status_class = "passed" if result["status"] == "passed" else "failed"
                duration_text = f"{result['duration']:.2f}ì´ˆ" if result['duration'] else "N/A"
                test_results_html += f'''
                <div class="test-item">
                    <span>{result["test_name"]}</span>
                    <div>
                        <span class="status-badge {status_class}">{result["status"].upper()}</span>
                        <span class="duration">{duration_text}</span>
                    </div>
                </div>
                '''
            test_results_html += "</div>"
        
        # ê¶Œì¥ì‚¬í•­ HTML ìƒì„±
        recommendations_html = ""
        for rec in report_data["recommendations"]:
            recommendations_html += f"<li>{rec}</li>"
        
        # HTML ìƒì„±
        html_content = html_template.format(
            total_tests=report_data["execution_summary"]["total_tests"],
            passed=report_data["execution_summary"]["passed"],
            failed=report_data["execution_summary"]["failed"],
            success_rate=report_data["execution_summary"]["success_rate"],
            duration=report_data["execution_summary"]["total_duration"] / 60,
            test_results_html=test_results_html,
            recommendations_html=recommendations_html,
            timestamp=report_data["execution_summary"]["timestamp"]
        )
        
        # HTML íŒŒì¼ ì €ì¥
        html_file = self.session_dir / "comprehensive_report.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"ğŸŒ HTML ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {html_file}")
    
    def run_all_tests(self, test_types: Optional[List[str]] = None) -> Dict[str, Any]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ VIBA AI ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        all_test_types = ["unit", "integration", "performance", "e2e", "security", "quality"]
        selected_types = test_types or all_test_types
        
        test_results = {}
        
        if "unit" in selected_types:
            test_results["unit"] = self.run_unit_tests()
        
        if "integration" in selected_types:
            test_results["integration"] = self.run_integration_tests()
        
        if "performance" in selected_types:
            test_results["performance"] = self.run_performance_tests()
        
        if "e2e" in selected_types:
            test_results["e2e"] = self.run_e2e_tests()
        
        if "security" in selected_types:
            test_results["security"] = self.run_security_tests()
        
        if "quality" in selected_types:
            test_results["quality"] = self.run_code_quality_tests()
        
        # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        comprehensive_report = self.generate_comprehensive_report()
        
        logger.info("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
        return comprehensive_report


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="VIBA AI ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    parser.add_argument(
        "--types", 
        nargs="*", 
        choices=["unit", "integration", "performance", "e2e", "security", "quality"],
        help="ì‹¤í–‰í•  í…ŒìŠ¤íŠ¸ íƒ€ì… (ê¸°ë³¸ê°’: ëª¨ë“  í…ŒìŠ¤íŠ¸)"
    )
    parser.add_argument("--timeout", type=int, default=600, help="í…ŒìŠ¤íŠ¸ íƒ€ì„ì•„ì›ƒ (ì´ˆ)")
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
    
    args = parser.parse_args()
    
    # ë¡œê¹… ë ˆë²¨ ì„¤ì •
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    config = {
        "test_timeout": args.timeout
    }
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    runner = VIBATestRunner(config)
    report = runner.run_all_tests(args.types)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ¤– VIBA AI ì¢…í•© í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)
    print(f"ì´ í…ŒìŠ¤íŠ¸: {report['execution_summary']['total_tests']}")
    print(f"ì„±ê³µ: {report['execution_summary']['passed']}")
    print(f"ì‹¤íŒ¨: {report['execution_summary']['failed']}")
    print(f"ì„±ê³µë¥ : {report['execution_summary']['success_rate']}")
    print(f"ì‹¤í–‰ ì‹œê°„: {report['execution_summary']['total_duration']:.2f}ì´ˆ")
    print(f"ë¦¬í¬íŠ¸: {runner.session_dir}/comprehensive_report.html")
    
    # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¢…ë£Œ ì½”ë“œ 1
    if report['execution_summary']['failed'] > 0:
        sys.exit(1)


if __name__ == "__main__":
    main()