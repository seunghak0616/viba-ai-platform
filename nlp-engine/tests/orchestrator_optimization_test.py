"""
ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ê³ ë„í™” í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸
=================================

VIBA AI ì½”ì–´ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì˜ ê³ ë„í™” ê¸°ëŠ¥ì„ ê²€ì¦í•˜ëŠ” ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ

@version 1.0
@author VIBA AI Team
@date 2025.07.06
"""

import asyncio
import json
import logging
import time
import numpy as np
import psutil
import gc
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, deque
import random
import statistics
from concurrent.futures import ThreadPoolExecutor

# í”„ë¡œì íŠ¸ ì„í¬íŠ¸
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

# ê³ ë„í™”ëœ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„í¬íŠ¸
try:
    from ai.advanced_orchestrator import (
        AdvancedOrchestrator, IntelligentAgentSelector, PredictiveScheduler,
        CollaborationOptimizer, PerformanceOptimizer, AgentPerformanceMetrics
    )
    ADVANCED_ORCHESTRATOR_AVAILABLE = True
except ImportError as e:
    print(f"ê³ ê¸‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    ADVANCED_ORCHESTRATOR_AVAILABLE = False

# ê¸°ë³¸ ì—ì´ì „íŠ¸ ì„í¬íŠ¸
try:
    from ai.agents.simple_test_agent import SimpleTestAgent
    from ai.base_agent import BaseVIBAAgent, AgentCapability
    BASIC_AGENTS_AVAILABLE = True
except ImportError as e:
    print(f"ê¸°ë³¸ ì—ì´ì „íŠ¸ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    BASIC_AGENTS_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""
    test_name: str
    execution_time: float
    success_rate: float
    throughput: float
    resource_usage: Dict[str, float]
    quality_metrics: Dict[str, float]
    improvement_rate: float = 0.0
    baseline_comparison: Dict[str, float] = field(default_factory=dict)


@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    response_times: List[float] = field(default_factory=list)
    success_counts: List[bool] = field(default_factory=list)
    resource_usage: List[Dict[str, float]] = field(default_factory=list)
    agent_utilization: List[Dict[str, float]] = field(default_factory=list)
    
    @property
    def avg_response_time(self) -> float:
        return np.mean(self.response_times) if self.response_times else 0.0
    
    @property
    def success_rate(self) -> float:
        return np.mean(self.success_counts) if self.success_counts else 0.0
    
    @property
    def throughput(self) -> float:
        if not self.response_times:
            return 0.0
        return len(self.response_times) / sum(self.response_times) if sum(self.response_times) > 0 else 0.0


class MockAgent(BaseVIBAAgent):
    """í…ŒìŠ¤íŠ¸ìš© ëª¨ì˜ ì—ì´ì „íŠ¸"""
    
    def __init__(self, agent_id: str, capabilities: List[AgentCapability], 
                 avg_execution_time: float = 1.0, success_rate: float = 0.9):
        super().__init__(agent_id, f"Mock {agent_id}", capabilities)
        self.avg_execution_time = avg_execution_time
        self.target_success_rate = success_rate
        self.call_count = 0
        
    async def initialize(self) -> bool:
        await asyncio.sleep(0.1)
        self.is_initialized = True
        return True
    
    async def execute_task(self, task) -> Dict[str, Any]:
        """ëª¨ì˜ ì‘ì—… ì‹¤í–‰"""
        self.call_count += 1
        
        # ì‹¤í–‰ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (ì •ê·œë¶„í¬)
        execution_time = max(0.1, np.random.normal(self.avg_execution_time, 0.2))
        await asyncio.sleep(execution_time)
        
        # ì„±ê³µ/ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜
        success = random.random() < self.target_success_rate
        
        return {
            "success": success,
            "agent_id": self.agent_id,
            "execution_time": execution_time,
            "call_count": self.call_count,
            "result": f"Mock result from {self.agent_id}" if success else None,
            "error": None if success else f"Simulated error in {self.agent_id}"
        }


class IntelligentAgentSelectorTest:
    """ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸ ì„ íƒê¸° í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.selector = IntelligentAgentSelector() if ADVANCED_ORCHESTRATOR_AVAILABLE else None
        self.test_agents = self._create_test_agents()
    
    def _create_test_agents(self) -> List[MockAgent]:
        """í…ŒìŠ¤íŠ¸ìš© ì—ì´ì „íŠ¸ ìƒì„±"""
        return [
            MockAgent("fast_agent", [AgentCapability.DESIGN_THEORY_APPLICATION], 0.5, 0.95),
            MockAgent("slow_agent", [AgentCapability.BIM_MODEL_GENERATION], 2.0, 0.99),
            MockAgent("unreliable_agent", [AgentCapability.PERFORMANCE_ANALYSIS], 1.0, 0.7),
            MockAgent("balanced_agent", [AgentCapability.DESIGN_REVIEW], 1.0, 0.9),
            MockAgent("specialist_agent", [AgentCapability.NATURAL_LANGUAGE_UNDERSTANDING], 1.5, 0.95)
        ]
    
    async def test_agent_selection_accuracy(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ì„ íƒ ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
        if not self.selector:
            return {"error": "ì§€ëŠ¥í˜• ì„ íƒê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ"}
        
        # ì—ì´ì „íŠ¸ ë“±ë¡
        for agent in self.test_agents:
            self.selector.register_agent(agent)
        
        test_cases = [
            {
                "required_capabilities": [AgentCapability.DESIGN_THEORY_APPLICATION],
                "complexity": 0.3,
                "expected_agents": ["fast_agent"],  # ë¹ ë¥¸ ì—ì´ì „íŠ¸ ì„ í˜¸
                "time_constraint": 1.0
            },
            {
                "required_capabilities": [AgentCapability.BIM_MODEL_GENERATION],
                "complexity": 0.8,
                "expected_agents": ["slow_agent"],  # ì •í™•ë„ ë†’ì€ ì—ì´ì „íŠ¸ ì„ í˜¸
                "time_constraint": None
            },
            {
                "required_capabilities": [AgentCapability.DESIGN_THEORY_APPLICATION, AgentCapability.BIM_MODEL_GENERATION],
                "complexity": 0.6,
                "expected_agents": ["fast_agent", "slow_agent"],  # ë‹¤ì¤‘ ì„ íƒ
                "time_constraint": 3.0
            }
        ]
        
        correct_selections = 0
        total_tests = len(test_cases)
        
        for i, test_case in enumerate(test_cases):
            selected_agents = self.selector.select_optimal_agents(
                test_case["required_capabilities"],
                test_case["complexity"],
                test_case.get("time_constraint")
            )
            
            # ì„ íƒ ì •í™•ë„ í‰ê°€
            expected_set = set(test_case["expected_agents"])
            selected_set = set(selected_agents)
            
            if expected_set.intersection(selected_set):  # í•˜ë‚˜ë¼ë„ ê²¹ì¹˜ë©´ ë¶€ë¶„ ì„±ê³µ
                correct_selections += 1
            
            print(f"  í…ŒìŠ¤íŠ¸ {i+1}: ì˜ˆìƒ={test_case['expected_agents']}, ì„ íƒ={selected_agents}")
        
        accuracy = correct_selections / total_tests
        
        return {
            "test_name": "agent_selection_accuracy",
            "accuracy": accuracy,
            "correct_selections": correct_selections,
            "total_tests": total_tests,
            "details": f"{correct_selections}/{total_tests} ì„ íƒì´ ì •í™•í•¨"
        }
    
    async def test_performance_learning(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í•™ìŠµ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸"""
        if not self.selector:
            return {"error": "ì§€ëŠ¥í˜• ì„ íƒê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ"}
        
        # ì—ì´ì „íŠ¸ ë“±ë¡ ë° ì´ˆê¸° ì„±ëŠ¥ ê¸°ë¡
        for agent in self.test_agents:
            self.selector.register_agent(agent)
            
            # ê°€ìƒ ì„±ëŠ¥ ì´ë ¥ ìƒì„±
            metrics = self.selector.performance_history[agent.agent_id]
            for _ in range(10):  # 10íšŒ ì‹¤í–‰ ì´ë ¥
                metrics.execution_times.append(agent.avg_execution_time + random.gauss(0, 0.2))
                metrics.success_rates.append(1.0 if random.random() < agent.target_success_rate else 0.0)
        
        # ì„±ëŠ¥ ê¸°ë°˜ ì„ íƒ í…ŒìŠ¤íŠ¸
        fast_requirements = [AgentCapability.DESIGN_THEORY_APPLICATION]
        selected_agents = self.selector.select_optimal_agents(fast_requirements, 0.5, 1.0)
        
        # fast_agentê°€ ì„ íƒë˜ì—ˆëŠ”ì§€ í™•ì¸ (ê°€ì¥ ë¹ ë¥¸ ì—ì´ì „íŠ¸)
        performance_aware = "fast_agent" in selected_agents
        
        return {
            "test_name": "performance_learning",
            "performance_aware_selection": performance_aware,
            "selected_agents": selected_agents,
            "agent_efficiency_scores": {
                agent_id: metrics.efficiency_score 
                for agent_id, metrics in self.selector.performance_history.items()
            }
        }


class PredictiveSchedulerTest:
    """ì˜ˆì¸¡ì  ìŠ¤ì¼€ì¤„ëŸ¬ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.scheduler = PredictiveScheduler() if ADVANCED_ORCHESTRATOR_AVAILABLE else None
    
    async def test_next_step_prediction(self) -> Dict[str, Any]:
        """ë‹¤ìŒ ë‹¨ê³„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"""
        if not self.scheduler:
            return {"error": "ì˜ˆì¸¡ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ"}
        
        test_scenarios = [
            {
                "current_state": {"outputs": {"design_concept": "modern_cafe"}},
                "execution_history": [],
                "expected_predictions": ["structural_analysis", "performance_evaluation"]
            },
            {
                "current_state": {"outputs": {"bim_model": "3d_model.ifc"}},
                "execution_history": [],
                "expected_predictions": ["code_compliance_check", "cost_estimation"]
            },
            {
                "current_state": {"outputs": {"performance_issues": ["high_energy_usage"]}},
                "execution_history": [],
                "expected_predictions": ["optimization_recommendations"]
            }
        ]
        
        prediction_accuracy = []
        
        for scenario in test_scenarios:
            predicted_steps = await self.scheduler.predict_next_steps(
                scenario["current_state"],
                scenario["execution_history"]
            )
            
            # ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚°
            expected_set = set(scenario["expected_predictions"])
            predicted_set = set(predicted_steps)
            
            if expected_set and predicted_set:
                intersection = len(expected_set.intersection(predicted_set))
                accuracy = intersection / len(expected_set)
            else:
                accuracy = 0.0
            
            prediction_accuracy.append(accuracy)
            
            print(f"  ì˜ˆìƒ: {scenario['expected_predictions']}")
            print(f"  ì˜ˆì¸¡: {predicted_steps}")
            print(f"  ì •í™•ë„: {accuracy:.2f}")
        
        avg_accuracy = np.mean(prediction_accuracy)
        
        return {
            "test_name": "next_step_prediction",
            "average_accuracy": avg_accuracy,
            "individual_accuracies": prediction_accuracy,
            "total_scenarios": len(test_scenarios)
        }


class CollaborationOptimizerTest:
    """í˜‘ë ¥ ìµœì í™”ê¸° í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.optimizer = CollaborationOptimizer() if ADVANCED_ORCHESTRATOR_AVAILABLE else None
    
    async def test_synergy_calculation(self) -> Dict[str, Any]:
        """ì‹œë„ˆì§€ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        if not self.optimizer:
            return {"error": "í˜‘ë ¥ ìµœì í™”ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ"}
        
        # ê°€ìƒ í˜‘ë ¥ ì‹œë‚˜ë¦¬ì˜¤ í•™ìŠµ
        collaboration_scenarios = [
            {
                "agents": ["design_theorist", "bim_specialist"],
                "metrics": {
                    "design_theorist_individual": 0.7,
                    "bim_specialist_individual": 0.8,
                    "combined_performance": 0.9  # ì‹œë„ˆì§€ íš¨ê³¼
                }
            },
            {
                "agents": ["performance_analyst", "design_reviewer"],
                "metrics": {
                    "performance_analyst_individual": 0.6,
                    "design_reviewer_individual": 0.7,
                    "combined_performance": 0.8  # ì‹œë„ˆì§€ íš¨ê³¼
                }
            },
            {
                "agents": ["fast_agent", "unreliable_agent"],
                "metrics": {
                    "fast_agent_individual": 0.8,
                    "unreliable_agent_individual": 0.5,
                    "combined_performance": 0.6  # ë¶€ì •ì  ì˜í–¥
                }
            }
        ]
        
        # í˜‘ë ¥ íŒ¨í„´ í•™ìŠµ
        for scenario in collaboration_scenarios:
            self.optimizer.learn_collaboration_patterns(
                scenario["agents"],
                scenario["metrics"]
            )
        
        # ì‹œë„ˆì§€ ì ìˆ˜ í™•ì¸
        synergy_scores = {}
        for scenario in collaboration_scenarios:
            if len(scenario["agents"]) >= 2:
                agent1, agent2 = scenario["agents"][:2]
                synergy = self.optimizer.synergy_matrix[agent1].get(agent2, 0)
                synergy_scores[f"{agent1}+{agent2}"] = synergy
        
        # ìµœì  ì¡°í•© í…ŒìŠ¤íŠ¸
        test_agents = ["design_theorist", "bim_specialist", "performance_analyst", "unreliable_agent"]
        optimal_combination = self.optimizer.optimize_agent_combination(test_agents)
        
        return {
            "test_name": "synergy_calculation",
            "synergy_scores": synergy_scores,
            "optimal_combination": optimal_combination,
            "synergy_matrix_size": len(self.optimizer.synergy_matrix),
            "collaboration_history_count": len(self.optimizer.collaboration_history)
        }


class PerformanceOptimizerTest:
    """ì„±ëŠ¥ ìµœì í™”ê¸° í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.optimizer = PerformanceOptimizer() if ADVANCED_ORCHESTRATOR_AVAILABLE else None
    
    async def test_performance_optimization(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        if not self.optimizer:
            return {"error": "ì„±ëŠ¥ ìµœì í™”ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ"}
        
        # ë‹¤ì–‘í•œ ì„±ëŠ¥ ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤
        problem_scenarios = [
            {
                "name": "slow_response",
                "metrics": {"response_time": 5.0, "success_rate": 0.9, "resource_usage": 0.6},
                "expected_optimizations": ["enable_caching", "parallel_execution"]
            },
            {
                "name": "low_success",
                "metrics": {"response_time": 1.0, "success_rate": 0.8, "resource_usage": 0.5},
                "expected_optimizations": ["add_validation", "increase_retries"]
            },
            {
                "name": "high_resource",
                "metrics": {"response_time": 1.5, "success_rate": 0.95, "resource_usage": 0.9},
                "expected_optimizations": ["optimize_algorithms", "batch_processing"]
            }
        ]
        
        optimization_results = []
        
        for scenario in problem_scenarios:
            result = await self.optimizer.optimize_workflow(
                scenario["metrics"],
                {"current_config": "default"}
            )
            
            applied_optimizations = result.get("applied_optimizations", [])
            expected_optimizations = scenario["expected_optimizations"]
            
            # ìµœì í™” ì ìš© ì •í™•ë„
            matching_optimizations = set(applied_optimizations).intersection(set(expected_optimizations))
            accuracy = len(matching_optimizations) / len(expected_optimizations) if expected_optimizations else 0
            
            optimization_results.append({
                "scenario": scenario["name"],
                "accuracy": accuracy,
                "applied": applied_optimizations,
                "expected": expected_optimizations,
                "improvements": result.get("expected_improvements", {})
            })
            
            print(f"  ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}")
            print(f"  ì ìš©ëœ ìµœì í™”: {applied_optimizations}")
            print(f"  ì •í™•ë„: {accuracy:.2f}")
        
        avg_accuracy = np.mean([r["accuracy"] for r in optimization_results])
        
        return {
            "test_name": "performance_optimization",
            "average_accuracy": avg_accuracy,
            "optimization_results": optimization_results,
            "total_scenarios": len(problem_scenarios)
        }


class AdvancedOrchestratorIntegrationTest:
    """ê³ ê¸‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í†µí•© í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.orchestrator = AdvancedOrchestrator() if ADVANCED_ORCHESTRATOR_AVAILABLE else None
        self.test_agents = self._create_test_agents()
        self.baseline_metrics = None
    
    def _create_test_agents(self) -> List[MockAgent]:
        """í…ŒìŠ¤íŠ¸ìš© ì—ì´ì „íŠ¸ ìƒì„±"""
        return [
            MockAgent("design_theorist", [AgentCapability.DESIGN_THEORY_APPLICATION], 1.0, 0.9),
            MockAgent("bim_specialist", [AgentCapability.BIM_MODEL_GENERATION], 1.5, 0.95),
            MockAgent("performance_analyst", [AgentCapability.PERFORMANCE_ANALYSIS], 2.0, 0.88),
            MockAgent("design_reviewer", [AgentCapability.DESIGN_REVIEW], 1.2, 0.92),
            MockAgent("architectural_specialist", [AgentCapability.NATURAL_LANGUAGE_UNDERSTANDING], 1.8, 0.9)
        ]
    
    async def setup_orchestrator(self):
        """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„¤ì •"""
        if not self.orchestrator:
            return False
        
        # í…ŒìŠ¤íŠ¸ ì—ì´ì „íŠ¸ ë“±ë¡
        for agent in self.test_agents:
            await self.orchestrator.register_agent(agent)
        
        return True
    
    async def test_intelligent_request_processing(self) -> Dict[str, Any]:
        """ì§€ëŠ¥í˜• ìš”ì²­ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        if not await self.setup_orchestrator():
            return {"error": "ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„¤ì • ì‹¤íŒ¨"}
        
        test_requests = [
            {
                "input": "ê°•ë‚¨ì— ê°„ë‹¨í•œ ì¹´í˜ë¥¼ ì„¤ê³„í•´ì¤˜",
                "complexity": "low",
                "expected_agents": 1,
                "max_time": 2.0
            },
            {
                "input": "3ì¸µ ë³µí•© ê±´ë¬¼ì„ ì¹œí™˜ê²½ ì¸ì¦ ë°›ì„ ìˆ˜ ìˆê²Œ ì„¤ê³„í•´ì¤˜",
                "complexity": "high",
                "expected_agents": 3,
                "max_time": 5.0
            },
            {
                "input": "í•œì˜¥ ìŠ¤íƒ€ì¼ ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤ ì„±ëŠ¥ ë¶„ì„í•´ì¤˜",
                "complexity": "medium",
                "expected_agents": 2,
                "max_time": 3.0
            }
        ]
        
        results = []
        
        for request in test_requests:
            start_time = time.time()
            
            result = await self.orchestrator.process_intelligent_request(
                request["input"],
                {"quality_level": "high"},
                "adaptive"
            )
            
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ ê²€ì¦
            success = result.get("success", False)
            agents_used = result.get("orchestration_metadata", {}).get("agents_used", [])
            
            test_result = {
                "request": request["input"],
                "success": success,
                "execution_time": execution_time,
                "agents_used": len(agents_used),
                "agents_list": agents_used,
                "within_time_limit": execution_time <= request["max_time"],
                "appropriate_agent_count": len(agents_used) <= request["expected_agents"] + 1  # ì—¬ìœ ë¶„ í—ˆìš©
            }
            
            results.append(test_result)
            
            print(f"  ìš”ì²­: {request['input'][:30]}...")
            print(f"  ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ")
            print(f"  ì‚¬ìš©ëœ ì—ì´ì „íŠ¸: {len(agents_used)}ê°œ")
            print(f"  ì„±ê³µ: {success}")
        
        # ì „ì²´ ì„±ëŠ¥ ì§‘ê³„
        avg_execution_time = np.mean([r["execution_time"] for r in results])
        success_rate = np.mean([r["success"] for r in results])
        time_compliance = np.mean([r["within_time_limit"] for r in results])
        
        return {
            "test_name": "intelligent_request_processing",
            "average_execution_time": avg_execution_time,
            "success_rate": success_rate,
            "time_compliance_rate": time_compliance,
            "total_requests": len(results),
            "detailed_results": results
        }
    
    async def test_performance_improvement(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ê°œì„  í…ŒìŠ¤íŠ¸"""
        if not await self.setup_orchestrator():
            return {"error": "ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„¤ì • ì‹¤íŒ¨"}
        
        # ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì • (ê¸°ë³¸ ì„¤ì •)
        baseline_request = "ì„œìš¸ì— ì‚¬ë¬´ìš© ë¹Œë”©ì„ ì„¤ê³„í•´ì¤˜"
        baseline_times = []
        
        for _ in range(5):  # 5íšŒ ë°˜ë³µ ì¸¡ì •
            start_time = time.time()
            result = await self.orchestrator.process_intelligent_request(
                baseline_request, {}, "sequential"  # ê¸°ë³¸ ìˆœì°¨ ì‹¤í–‰
            )
            execution_time = time.time() - start_time
            
            if result.get("success", False):
                baseline_times.append(execution_time)
        
        baseline_avg = np.mean(baseline_times) if baseline_times else 0
        
        # ìµœì í™”ëœ ì‹¤í–‰ ì¸¡ì •
        optimized_times = []
        
        for _ in range(5):  # 5íšŒ ë°˜ë³µ ì¸¡ì •
            start_time = time.time()
            result = await self.orchestrator.process_intelligent_request(
                baseline_request, {"quality_level": "high"}, "adaptive"  # ì ì‘í˜• ì‹¤í–‰
            )
            execution_time = time.time() - start_time
            
            if result.get("success", False):
                optimized_times.append(execution_time)
        
        optimized_avg = np.mean(optimized_times) if optimized_times else 0
        
        # ê°œì„ ìœ¨ ê³„ì‚°
        if baseline_avg > 0:
            improvement_rate = (baseline_avg - optimized_avg) / baseline_avg * 100
        else:
            improvement_rate = 0
        
        return {
            "test_name": "performance_improvement",
            "baseline_avg_time": baseline_avg,
            "optimized_avg_time": optimized_avg,
            "improvement_rate_percent": improvement_rate,
            "baseline_measurements": len(baseline_times),
            "optimized_measurements": len(optimized_times),
            "statistical_significance": abs(improvement_rate) > 10  # 10% ì´ìƒ ê°œì„ ì„ ìœ ì˜ë¯¸ë¡œ íŒë‹¨
        }


class StressTestSuite:
    """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self):
        self.orchestrator = AdvancedOrchestrator() if ADVANCED_ORCHESTRATOR_AVAILABLE else None
        self.test_agents = self._create_test_agents()
    
    def _create_test_agents(self) -> List[MockAgent]:
        """ê³ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© ì—ì´ì „íŠ¸ ìƒì„±"""
        return [
            MockAgent("fast_agent_1", [AgentCapability.DESIGN_THEORY_APPLICATION], 0.3, 0.95),
            MockAgent("fast_agent_2", [AgentCapability.BIM_MODEL_GENERATION], 0.5, 0.93),
            MockAgent("fast_agent_3", [AgentCapability.PERFORMANCE_ANALYSIS], 0.4, 0.92),
            MockAgent("fast_agent_4", [AgentCapability.DESIGN_REVIEW], 0.6, 0.94)
        ]
    
    async def setup_stress_test(self):
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        if not self.orchestrator:
            return False
        
        for agent in self.test_agents:
            await self.orchestrator.register_agent(agent)
        
        return True
    
    async def test_concurrent_load(self, concurrent_requests: int = 50, duration: int = 30) -> Dict[str, Any]:
        """ë™ì‹œ ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        if not await self.setup_stress_test():
            return {"error": "ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì„¤ì • ì‹¤íŒ¨"}
        
        test_requests = [
            "ê°•ë‚¨ì— ì¹´í˜ë¥¼ ì„¤ê³„í•´ì¤˜",
            "ì‚¬ë¬´ìš© ë¹Œë”© ì„±ëŠ¥ ë¶„ì„í•´ì¤˜",
            "ì£¼íƒ ì„¤ê³„ ê²€í† í•´ì¤˜",
            "í•œì˜¥ ìŠ¤íƒ€ì¼ ê±´ë¬¼ ì„¤ê³„í•´ì¤˜",
            "ë³µí•© ê±´ë¬¼ BIM ëª¨ë¸ ìƒì„±í•´ì¤˜"
        ]
        
        metrics = PerformanceMetrics()
        start_time = time.time()
        completed_requests = 0
        errors = []
        
        async def process_single_request(request_id: int):
            """ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬"""
            nonlocal completed_requests
            
            try:
                request_start = time.time()
                request_text = random.choice(test_requests)
                
                result = await self.orchestrator.process_intelligent_request(
                    request_text, {}, "adaptive"
                )
                
                request_time = time.time() - request_start
                success = result.get("success", False)
                
                # ë©”íŠ¸ë¦­ ê¸°ë¡
                metrics.response_times.append(request_time)
                metrics.success_counts.append(success)
                
                # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ê¸°ë¡
                process = psutil.Process()
                resource_usage = {
                    "cpu_percent": process.cpu_percent(),
                    "memory_mb": process.memory_info().rss / 1024 / 1024
                }
                metrics.resource_usage.append(resource_usage)
                
                completed_requests += 1
                
            except Exception as e:
                errors.append(str(e))
        
        # ë™ì‹œ ìš”ì²­ ìƒì„± ë° ì‹¤í–‰
        tasks = []
        for i in range(concurrent_requests):
            task = asyncio.create_task(process_single_request(i))
            tasks.append(task)
            
            # ì§§ì€ ê°„ê²©ìœ¼ë¡œ ìš”ì²­ ì‹œì‘ (ì‹¤ì œ ë¶€í•˜ ì‹œë®¬ë ˆì´ì…˜)
            await asyncio.sleep(0.1)
        
        # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ duration ì´ˆ)
        try:
            await asyncio.wait_for(asyncio.gather(*tasks, return_exceptions=True), timeout=duration)
        except asyncio.TimeoutError:
            # íƒ€ì„ì•„ì›ƒ ì‹œ ë¯¸ì™„ë£Œ ì‘ì—… ì·¨ì†Œ
            for task in tasks:
                if not task.done():
                    task.cancel()
        
        total_time = time.time() - start_time
        
        # ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
        process = psutil.Process()
        final_memory = process.memory_info().rss / 1024 / 1024
        
        return {
            "test_name": "concurrent_load_test",
            "concurrent_requests": concurrent_requests,
            "completed_requests": completed_requests,
            "completion_rate": completed_requests / concurrent_requests,
            "total_duration": total_time,
            "average_response_time": metrics.avg_response_time,
            "success_rate": metrics.success_rate,
            "throughput_rps": completed_requests / total_time if total_time > 0 else 0,
            "peak_memory_mb": final_memory,
            "error_count": len(errors),
            "errors": errors[:5]  # ìµœëŒ€ 5ê°œ ì—ëŸ¬ë§Œ í‘œì‹œ
        }
    
    async def test_memory_stability(self, iterations: int = 100) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
        if not await self.setup_stress_test():
            return {"error": "ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì„¤ì • ì‹¤íŒ¨"}
        
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
        memory_samples = [initial_memory]
        
        for i in range(iterations):
            # ìš”ì²­ ì²˜ë¦¬
            result = await self.orchestrator.process_intelligent_request(
                f"í…ŒìŠ¤íŠ¸ ìš”ì²­ {i+1}", {}, "adaptive"
            )
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            current_memory = psutil.Process().memory_info().rss / 1024 / 1024
            memory_samples.append(current_memory)
            
            # 10íšŒë§ˆë‹¤ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            if (i + 1) % 10 == 0:
                gc.collect()
                
                # GC í›„ ë©”ëª¨ë¦¬ ì¸¡ì •
                gc_memory = psutil.Process().memory_info().rss / 1024 / 1024
                memory_samples.append(gc_memory)
        
        final_memory = memory_samples[-1]
        memory_increase = final_memory - initial_memory
        max_memory = max(memory_samples)
        
        # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€ (ì„ í˜• ì¦ê°€ ê²½í–¥)
        if len(memory_samples) > 10:
            # ì„ í˜• íšŒê·€ë¡œ ì¦ê°€ ê²½í–¥ ë¶„ì„
            x = np.arange(len(memory_samples))
            y = np.array(memory_samples)
            slope = np.polyfit(x, y, 1)[0]
            
            memory_leak_detected = slope > 1.0  # 1MB/iteration ì´ìƒ ì¦ê°€ ì‹œ ëˆ„ìˆ˜ë¡œ íŒë‹¨
        else:
            memory_leak_detected = False
        
        return {
            "test_name": "memory_stability_test",
            "iterations": iterations,
            "initial_memory_mb": initial_memory,
            "final_memory_mb": final_memory,
            "memory_increase_mb": memory_increase,
            "peak_memory_mb": max_memory,
            "memory_leak_detected": memory_leak_detected,
            "memory_samples": memory_samples[-10:],  # ë§ˆì§€ë§‰ 10ê°œ ìƒ˜í”Œë§Œ
            "stability_rating": "stable" if memory_increase < 50 else "unstable"
        }


class OrchestratorOptimizationTestSuite:
    """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìµœì í™” ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self):
        self.test_results = []
        self.performance_baselines = {}
        
    async def run_all_optimization_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        print("=" * 70)
        
        start_time = time.time()
        
        # 1. ì»´í¬ë„ŒíŠ¸ë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
        await self._run_component_tests()
        
        # 2. í†µí•© í…ŒìŠ¤íŠ¸
        await self._run_integration_tests()
        
        # 3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        await self._run_performance_benchmarks()
        
        # 4. ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
        await self._run_stress_tests()
        
        # 5. ê²°ê³¼ ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±
        summary = await self._generate_comprehensive_report()
        
        total_time = time.time() - start_time
        summary["total_execution_time"] = total_time
        
        print("\n" + "=" * 70)
        print("ğŸ“Š ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìµœì í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {len(self.test_results)}")
        
        # ì„±ê³µë¥  ê³„ì‚°
        successful_tests = sum(1 for r in self.test_results if r.get("success", True))
        success_rate = successful_tests / len(self.test_results) * 100 if self.test_results else 0
        
        print(f"ì„±ê³µë¥ : {success_rate:.1f}% ({successful_tests}/{len(self.test_results)})")
        
        if success_rate >= 80:
            print("âœ… ìµœì í™” í…ŒìŠ¤íŠ¸ í†µê³¼! ì‹œìŠ¤í…œì´ ì˜ˆìƒëŒ€ë¡œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("âŒ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ì¶”ê°€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return summary
    
    async def _run_component_tests(self):
        """ì»´í¬ë„ŒíŠ¸ë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""
        print("\n1ï¸âƒ£ ì»´í¬ë„ŒíŠ¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸")
        print("-" * 50)
        
        if not ADVANCED_ORCHESTRATOR_AVAILABLE:
            print("  âš ï¸ ê³ ê¸‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        # ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸ ì„ íƒê¸° í…ŒìŠ¤íŠ¸
        print("  ğŸ§  ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸ ì„ íƒê¸° í…ŒìŠ¤íŠ¸...")
        selector_test = IntelligentAgentSelectorTest()
        
        accuracy_result = await selector_test.test_agent_selection_accuracy()
        self.test_results.append(accuracy_result)
        print(f"    ì„ íƒ ì •í™•ë„: {accuracy_result.get('accuracy', 0):.2f}")
        
        learning_result = await selector_test.test_performance_learning()
        self.test_results.append(learning_result)
        print(f"    ì„±ëŠ¥ ì¸ì‹ ì„ íƒ: {learning_result.get('performance_aware_selection', False)}")
        
        # ì˜ˆì¸¡ì  ìŠ¤ì¼€ì¤„ëŸ¬ í…ŒìŠ¤íŠ¸
        print("  ğŸ”® ì˜ˆì¸¡ì  ìŠ¤ì¼€ì¤„ëŸ¬ í…ŒìŠ¤íŠ¸...")
        scheduler_test = PredictiveSchedulerTest()
        
        prediction_result = await scheduler_test.test_next_step_prediction()
        self.test_results.append(prediction_result)
        print(f"    ì˜ˆì¸¡ ì •í™•ë„: {prediction_result.get('average_accuracy', 0):.2f}")
        
        # í˜‘ë ¥ ìµœì í™”ê¸° í…ŒìŠ¤íŠ¸
        print("  ğŸ¤ í˜‘ë ¥ ìµœì í™”ê¸° í…ŒìŠ¤íŠ¸...")
        collaboration_test = CollaborationOptimizerTest()
        
        synergy_result = await collaboration_test.test_synergy_calculation()
        self.test_results.append(synergy_result)
        print(f"    ì‹œë„ˆì§€ ë§¤íŠ¸ë¦­ìŠ¤ í¬ê¸°: {synergy_result.get('synergy_matrix_size', 0)}")
        
        # ì„±ëŠ¥ ìµœì í™”ê¸° í…ŒìŠ¤íŠ¸
        print("  âš¡ ì„±ëŠ¥ ìµœì í™”ê¸° í…ŒìŠ¤íŠ¸...")
        performance_test = PerformanceOptimizerTest()
        
        optimization_result = await performance_test.test_performance_optimization()
        self.test_results.append(optimization_result)
        print(f"    ìµœì í™” ì •í™•ë„: {optimization_result.get('average_accuracy', 0):.2f}")
    
    async def _run_integration_tests(self):
        """í†µí•© í…ŒìŠ¤íŠ¸"""
        print("\n2ï¸âƒ£ í†µí•© í…ŒìŠ¤íŠ¸")
        print("-" * 50)
        
        if not ADVANCED_ORCHESTRATOR_AVAILABLE:
            print("  âš ï¸ ê³ ê¸‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        integration_test = AdvancedOrchestratorIntegrationTest()
        
        # ì§€ëŠ¥í˜• ìš”ì²­ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        print("  ğŸ¯ ì§€ëŠ¥í˜• ìš”ì²­ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        processing_result = await integration_test.test_intelligent_request_processing()
        self.test_results.append(processing_result)
        
        avg_time = processing_result.get('average_execution_time', 0)
        success_rate = processing_result.get('success_rate', 0)
        print(f"    í‰ê·  ì‹¤í–‰ì‹œê°„: {avg_time:.2f}ì´ˆ")
        print(f"    ì„±ê³µë¥ : {success_rate:.2f}")
        
        # ì„±ëŠ¥ ê°œì„  í…ŒìŠ¤íŠ¸
        print("  ğŸ“ˆ ì„±ëŠ¥ ê°œì„  í…ŒìŠ¤íŠ¸...")
        improvement_result = await integration_test.test_performance_improvement()
        self.test_results.append(improvement_result)
        
        improvement_rate = improvement_result.get('improvement_rate_percent', 0)
        print(f"    ì„±ëŠ¥ ê°œì„ ë¥ : {improvement_rate:.1f}%")
        
        # ë² ì´ìŠ¤ë¼ì¸ ì €ì¥
        self.performance_baselines = {
            "baseline_time": improvement_result.get('baseline_avg_time', 0),
            "optimized_time": improvement_result.get('optimized_avg_time', 0)
        }
    
    async def _run_performance_benchmarks(self):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print("\n3ï¸âƒ£ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
        print("-" * 50)
        
        if not ADVANCED_ORCHESTRATOR_AVAILABLE:
            print("  âš ï¸ ê³ ê¸‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        stress_test = StressTestSuite()
        
        # ì¤‘ê°„ ë¶€í•˜ í…ŒìŠ¤íŠ¸
        print("  âš¡ ì¤‘ê°„ ë¶€í•˜ í…ŒìŠ¤íŠ¸ (20 ë™ì‹œ ìš”ì²­)...")
        medium_load_result = await stress_test.test_concurrent_load(concurrent_requests=20, duration=30)
        self.test_results.append(medium_load_result)
        
        throughput = medium_load_result.get('throughput_rps', 0)
        completion_rate = medium_load_result.get('completion_rate', 0)
        print(f"    ì²˜ë¦¬ëŸ‰: {throughput:.1f} RPS")
        print(f"    ì™„ë£Œìœ¨: {completion_rate:.2f}")
        
        # ë©”ëª¨ë¦¬ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
        print("  ğŸ’¾ ë©”ëª¨ë¦¬ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸...")
        memory_result = await stress_test.test_memory_stability(iterations=50)
        self.test_results.append(memory_result)
        
        memory_increase = memory_result.get('memory_increase_mb', 0)
        stability = memory_result.get('stability_rating', 'unknown')
        print(f"    ë©”ëª¨ë¦¬ ì¦ê°€: {memory_increase:.1f}MB")
        print(f"    ì•ˆì •ì„±: {stability}")
    
    async def _run_stress_tests(self):
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
        print("\n4ï¸âƒ£ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸")
        print("-" * 50)
        
        if not ADVANCED_ORCHESTRATOR_AVAILABLE:
            print("  âš ï¸ ê³ ê¸‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        stress_test = StressTestSuite()
        
        # ë†’ì€ ë¶€í•˜ í…ŒìŠ¤íŠ¸
        print("  ğŸ”¥ ê³ ë¶€í•˜ í…ŒìŠ¤íŠ¸ (50 ë™ì‹œ ìš”ì²­)...")
        high_load_result = await stress_test.test_concurrent_load(concurrent_requests=50, duration=45)
        self.test_results.append(high_load_result)
        
        throughput = high_load_result.get('throughput_rps', 0)
        success_rate = high_load_result.get('success_rate', 0)
        error_count = high_load_result.get('error_count', 0)
        
        print(f"    ì²˜ë¦¬ëŸ‰: {throughput:.1f} RPS")
        print(f"    ì„±ê³µë¥ : {success_rate:.2f}")
        print(f"    ì—ëŸ¬ ìˆ˜: {error_count}ê°œ")
        
        # ëª©í‘œ ì„±ëŠ¥ ê²€ì¦
        target_throughput = 10.0  # 10 RPS ëª©í‘œ
        target_success_rate = 0.9   # 90% ì„±ê³µë¥  ëª©í‘œ
        
        throughput_ok = throughput >= target_throughput
        success_rate_ok = success_rate >= target_success_rate
        
        print(f"    ëª©í‘œ ë‹¬ì„±: ì²˜ë¦¬ëŸ‰ {'âœ…' if throughput_ok else 'âŒ'}, ì„±ê³µë¥  {'âœ…' if success_rate_ok else 'âŒ'}")
    
    async def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼ ë¶„ë¥˜
        component_tests = []
        integration_tests = []
        performance_tests = []
        stress_tests = []
        
        for result in self.test_results:
            test_name = result.get('test_name', '')
            
            if any(keyword in test_name for keyword in ['selection', 'prediction', 'synergy', 'optimization']):
                component_tests.append(result)
            elif any(keyword in test_name for keyword in ['intelligent', 'improvement']):
                integration_tests.append(result)
            elif any(keyword in test_name for keyword in ['memory', 'stability']):
                performance_tests.append(result)
            elif any(keyword in test_name for keyword in ['concurrent', 'load', 'stress']):
                stress_tests.append(result)
        
        # í•µì‹¬ ë©”íŠ¸ë¦­ ê³„ì‚°
        key_metrics = self._calculate_key_metrics()
        
        # ê°œì„  ì‚¬í•­ ì‹ë³„
        improvements = self._identify_improvements()
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations()
        
        comprehensive_report = {
            "test_summary": {
                "total_tests": len(self.test_results),
                "component_tests": len(component_tests),
                "integration_tests": len(integration_tests),
                "performance_tests": len(performance_tests),
                "stress_tests": len(stress_tests)
            },
            
            "key_metrics": key_metrics,
            "improvements_identified": improvements,
            "recommendations": recommendations,
            
            "detailed_results": {
                "component_tests": component_tests,
                "integration_tests": integration_tests,
                "performance_tests": performance_tests,
                "stress_tests": stress_tests
            },
            
            "performance_baselines": self.performance_baselines,
            "test_execution_summary": {
                "advanced_orchestrator_available": ADVANCED_ORCHESTRATOR_AVAILABLE,
                "basic_agents_available": BASIC_AGENTS_AVAILABLE,
                "test_completion_time": time.strftime("%Y-%m-%d %H:%M:%S")
            }
        }
        
        return comprehensive_report
    
    def _calculate_key_metrics(self) -> Dict[str, Any]:
        """í•µì‹¬ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {
            "agent_selection_accuracy": 0.0,
            "prediction_accuracy": 0.0,
            "optimization_accuracy": 0.0,
            "average_response_time": 0.0,
            "system_throughput": 0.0,
            "memory_stability": "unknown",
            "overall_performance_score": 0.0
        }
        
        # ê° í…ŒìŠ¤íŠ¸ ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ
        for result in self.test_results:
            test_name = result.get('test_name', '')
            
            if 'agent_selection' in test_name:
                metrics["agent_selection_accuracy"] = result.get('accuracy', 0.0)
            elif 'prediction' in test_name:
                metrics["prediction_accuracy"] = result.get('average_accuracy', 0.0)
            elif 'optimization' in test_name:
                metrics["optimization_accuracy"] = result.get('average_accuracy', 0.0)
            elif 'intelligent_request' in test_name:
                metrics["average_response_time"] = result.get('average_execution_time', 0.0)
            elif 'concurrent_load' in test_name:
                metrics["system_throughput"] = result.get('throughput_rps', 0.0)
            elif 'memory_stability' in test_name:
                metrics["memory_stability"] = result.get('stability_rating', 'unknown')
        
        # ì „ì²´ ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        performance_components = [
            metrics["agent_selection_accuracy"] * 0.2,
            metrics["prediction_accuracy"] * 0.2,
            metrics["optimization_accuracy"] * 0.2,
            (1.0 / max(metrics["average_response_time"], 0.1)) * 0.2,  # ì‹œê°„ì€ ì—­ìˆ˜ë¡œ ê³„ì‚°
            min(metrics["system_throughput"] / 10.0, 1.0) * 0.2  # 10 RPSë¥¼ 1.0ìœ¼ë¡œ ì •ê·œí™”
        ]
        
        metrics["overall_performance_score"] = sum(performance_components)
        
        return metrics
    
    def _identify_improvements(self) -> List[str]:
        """ê°œì„  ì‚¬í•­ ì‹ë³„"""
        improvements = []
        
        # ì„±ëŠ¥ ê°œì„ ë¥  í™•ì¸
        baseline_time = self.performance_baselines.get('baseline_time', 0)
        optimized_time = self.performance_baselines.get('optimized_time', 0)
        
        if baseline_time > 0 and optimized_time > 0:
            improvement_rate = (baseline_time - optimized_time) / baseline_time * 100
            if improvement_rate > 10:
                improvements.append(f"ì‘ë‹µ ì‹œê°„ {improvement_rate:.1f}% ê°œì„ ")
            elif improvement_rate < -10:
                improvements.append(f"ì‘ë‹µ ì‹œê°„ {abs(improvement_rate):.1f}% ì €í•˜")
        
        # ê° ì»´í¬ë„ŒíŠ¸ë³„ ì„±ëŠ¥ í™•ì¸
        for result in self.test_results:
            test_name = result.get('test_name', '')
            
            if 'agent_selection' in test_name:
                accuracy = result.get('accuracy', 0)
                if accuracy >= 0.9:
                    improvements.append("ì—ì´ì „íŠ¸ ì„ íƒ ì •í™•ë„ ìš°ìˆ˜")
                elif accuracy < 0.7:
                    improvements.append("ì—ì´ì „íŠ¸ ì„ íƒ ì •í™•ë„ ê°œì„  í•„ìš”")
            
            elif 'concurrent_load' in test_name:
                throughput = result.get('throughput_rps', 0)
                if throughput >= 10:
                    improvements.append("ë†’ì€ ì²˜ë¦¬ëŸ‰ ë‹¬ì„±")
                elif throughput < 5:
                    improvements.append("ì²˜ë¦¬ëŸ‰ ê°œì„  í•„ìš”")
            
            elif 'memory_stability' in test_name:
                stability = result.get('stability_rating', '')
                if stability == 'stable':
                    improvements.append("ë©”ëª¨ë¦¬ ì•ˆì •ì„± í™•ë³´")
                elif stability == 'unstable':
                    improvements.append("ë©”ëª¨ë¦¬ ì•ˆì •ì„± ê°œì„  í•„ìš”")
        
        return improvements
    
    def _generate_recommendations(self) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        key_metrics = self._calculate_key_metrics()
        
        if key_metrics["agent_selection_accuracy"] < 0.8:
            recommendations.append("ì—ì´ì „íŠ¸ ì„ íƒ ì•Œê³ ë¦¬ì¦˜ ê°œì„  - ë” ë§ì€ í•™ìŠµ ë°ì´í„° í•„ìš”")
        
        if key_metrics["prediction_accuracy"] < 0.7:
            recommendations.append("ì˜ˆì¸¡ ëª¨ë¸ ì¬í›ˆë ¨ - ì‹¤í–‰ íŒ¨í„´ ë°ì´í„° ìˆ˜ì§‘ ê°•í™”")
        
        if key_metrics["average_response_time"] > 2.0:
            recommendations.append("ì‘ë‹µ ì‹œê°„ ìµœì í™” - ìºì‹± ë° ë³‘ë ¬ ì²˜ë¦¬ ê°•í™”")
        
        if key_metrics["system_throughput"] < 5.0:
            recommendations.append("ì‹œìŠ¤í…œ ì²˜ë¦¬ëŸ‰ í–¥ìƒ - ë¦¬ì†ŒìŠ¤ í™•ì¥ ë° ì•Œê³ ë¦¬ì¦˜ ìµœì í™”")
        
        if key_metrics["memory_stability"] == "unstable":
            recommendations.append("ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ìˆ˜ì • - ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”")
        
        if key_metrics["overall_performance_score"] < 0.7:
            recommendations.append("ì „ì²´ì ì¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì¬ê²€í†  í•„ìš”")
        
        # ì¼ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­
        recommendations.extend([
            "ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•",
            "A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ì ì§„ì  ê°œì„ ",
            "ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ë° ë°˜ì˜",
            "ì •ê¸°ì ì¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤ì‹œ"
        ])
        
        return recommendations


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    
    print("ğŸ¯ VIBA AI ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ê³ ë„í™” í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸")
    print("=" * 70)
    print("ì´ í…ŒìŠ¤íŠ¸ëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì˜ ìµœì í™” ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤.")
    print()
    
    # í™˜ê²½ ìƒíƒœ í™•ì¸
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ í™˜ê²½ í™•ì¸:")
    print(f"  ê³ ê¸‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if ADVANCED_ORCHESTRATOR_AVAILABLE else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
    print(f"  ê¸°ë³¸ ì—ì´ì „íŠ¸: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if BASIC_AGENTS_AVAILABLE else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
    print(f"  ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {psutil.virtual_memory().total / 1024 / 1024 / 1024:.1f}GB")
    print(f"  CPU ì½”ì–´: {psutil.cpu_count()}ê°œ")
    print()
    
    # ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_suite = OrchestratorOptimizationTestSuite()
    comprehensive_results = await test_suite.run_all_optimization_tests()
    
    # ê²°ê³¼ ì €ì¥
    try:
        os.makedirs("test_results", exist_ok=True)
        
        # JSON ê²°ê³¼ ì €ì¥
        with open("test_results/orchestrator_optimization_results.json", "w", encoding="utf-8") as f:
            json.dump(comprehensive_results, f, indent=2, ensure_ascii=False, default=str)
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
        summary_report = f"""
# VIBA AI ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìµœì í™” í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸

## ğŸ“Š ì „ì²´ ìš”ì•½
- ì´ í…ŒìŠ¤íŠ¸: {comprehensive_results['test_summary']['total_tests']}ê°œ
- ì‹¤í–‰ ì‹œê°„: {comprehensive_results.get('total_execution_time', 0):.2f}ì´ˆ
- ì „ì²´ ì„±ëŠ¥ ì ìˆ˜: {comprehensive_results['key_metrics']['overall_performance_score']:.2f}/1.0

## ğŸ¯ í•µì‹¬ ë©”íŠ¸ë¦­
- ì—ì´ì „íŠ¸ ì„ íƒ ì •í™•ë„: {comprehensive_results['key_metrics']['agent_selection_accuracy']:.2f}
- ì˜ˆì¸¡ ì •í™•ë„: {comprehensive_results['key_metrics']['prediction_accuracy']:.2f}
- í‰ê·  ì‘ë‹µ ì‹œê°„: {comprehensive_results['key_metrics']['average_response_time']:.2f}ì´ˆ
- ì‹œìŠ¤í…œ ì²˜ë¦¬ëŸ‰: {comprehensive_results['key_metrics']['system_throughput']:.1f} RPS

## ğŸš€ ì‹ë³„ëœ ê°œì„ ì‚¬í•­
{chr(10).join('- ' + improvement for improvement in comprehensive_results['improvements_identified'])}

## ğŸ’¡ ê¶Œì¥ì‚¬í•­
{chr(10).join('- ' + rec for rec in comprehensive_results['recommendations'])}

---
*í…ŒìŠ¤íŠ¸ ì™„ë£Œ ì‹œê°„: {comprehensive_results['test_execution_summary']['test_completion_time']}*
        """
        
        with open("test_results/orchestrator_optimization_summary.md", "w", encoding="utf-8") as f:
            f.write(summary_report)
        
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ test_results/ ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"  - orchestrator_optimization_results.json: ìƒì„¸ ê²°ê³¼")
        print(f"  - orchestrator_optimization_summary.md: ìš”ì•½ ë¦¬í¬íŠ¸")
        
    except Exception as e:
        print(f"âš ï¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # ìµœì¢… ê¶Œì¥ì‚¬í•­ ì¶œë ¥
    print("\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­:")
    for i, recommendation in enumerate(comprehensive_results['recommendations'][:5], 1):
        print(f"  {i}. {recommendation}")
    
    return comprehensive_results


if __name__ == "__main__":
    asyncio.run(main())