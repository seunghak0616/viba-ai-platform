"""
Playwright MCPë¥¼ í™œìš©í•œ E2E í…ŒìŠ¤íŠ¸
=================================

VIBA AI ì‹œìŠ¤í…œì˜ ì—”ë“œíˆ¬ì—”ë“œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ Playwright í†µí•© í…ŒìŠ¤íŠ¸

@version 1.0
@author VIBA AI Team
@date 2025.07.06
"""

import asyncio
import sys
import os
import time
import json
from typing import Dict, Any, List
from dataclasses import dataclass
import logging

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

# MCP í†µí•© ì‹œìŠ¤í…œ
try:
    from ai.mcp_integration.claude_code_integration import ClaudeCodeIntegration
    MCP_AVAILABLE = True
except ImportError:
    MCP_AVAILABLE = False
    print("MCP í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš©ë¶ˆê°€ - ê¸°ë³¸ í…ŒìŠ¤íŠ¸ë¡œ ì§„í–‰")

# ì„±ëŠ¥ ìµœì í™” ëª¨ë“ˆ
from optimization.performance_optimizer import performance_optimizer, performance_monitor

# VIBA AI ì‹œìŠ¤í…œ
from ai.advanced_orchestrator import AdvancedOrchestrator
from ai.agents.materials_specialist import MaterialsSpecialistAgent

logger = logging.getLogger(__name__)


@dataclass
class TestCase:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
    name: str
    description: str
    input_data: Dict[str, Any]
    expected_output: Dict[str, Any]
    timeout: float = 30.0
    retry_count: int = 3


class PlaywrightE2ETestSuite:
    """Playwright E2E í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self):
        self.orchestrator = None
        self.mcp_integration = None
        self.test_results = []
        
        # í…ŒìŠ¤íŠ¸ ì„¤ì •
        self.config = {
            "timeout": 30000,  # 30ì´ˆ
            "viewport": {"width": 1920, "height": 1080},
            "user_agent": "VIBA-AI-Test-Suite/1.0"
        }
        
        # ì„±ëŠ¥ ì„ê³„ê°’
        self.performance_thresholds = {
            "response_time": 5.0,  # 5ì´ˆ
            "memory_usage": 500.0,  # 500MB
            "cpu_usage": 80.0  # 80%
        }
    
    async def setup(self):
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •"""
        logger.info("E2E í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì¤‘...")
        
        # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”
        self.orchestrator = AdvancedOrchestrator()
        
        # ê¸°ë³¸ ì—ì´ì „íŠ¸ ë“±ë¡
        materials_specialist = MaterialsSpecialistAgent()
        await self.orchestrator.register_agent(materials_specialist)
        
        # MCP í†µí•© ì„¤ì •
        if MCP_AVAILABLE:
            self.mcp_integration = ClaudeCodeIntegration()
            await self.mcp_integration.initialize()
            logger.info("MCP í†µí•© ì„¤ì • ì™„ë£Œ")
        
        logger.info("E2E í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì™„ë£Œ")
    
    def define_test_cases(self) -> List[TestCase]:
        """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜"""
        return [
            TestCase(
                name="basic_material_recommendation",
                description="ê¸°ë³¸ ì¬ë£Œ ì¶”ì²œ í…ŒìŠ¤íŠ¸",
                input_data={
                    "user_input": "ì¹œí™˜ê²½ ì£¼íƒìš© ë‹¨ì—´ì¬ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”",
                    "context": {"building_type": "residential"}
                },
                expected_output={
                    "success": True,
                    "has_recommendations": True,
                    "response_time": 5.0
                }
            ),
            TestCase(
                name="complex_building_design",
                description="ë³µì¡í•œ ê±´ë¬¼ ì„¤ê³„ í…ŒìŠ¤íŠ¸",
                input_data={
                    "user_input": "20ì¸µ ìƒì—…ìš© ê±´ë¬¼ì„ ì¹œí™˜ê²½ì ìœ¼ë¡œ ì„¤ê³„í•´ì£¼ì„¸ìš”",
                    "context": {
                        "building_type": "commercial",
                        "floors": 20,
                        "sustainability_priority": "high"
                    }
                },
                expected_output={
                    "success": True,
                    "has_design_elements": True,
                    "response_time": 10.0
                }
            ),
            TestCase(
                name="concurrent_requests",
                description="ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸",
                input_data={
                    "concurrent_count": 5,
                    "user_input": "ê°„ë‹¨í•œ ì£¼íƒ ì„¤ê³„",
                    "context": {"building_type": "residential"}
                },
                expected_output={
                    "all_success": True,
                    "avg_response_time": 3.0
                }
            ),
            TestCase(
                name="stress_test",
                description="ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸",
                input_data={
                    "request_count": 20,
                    "user_input": "ë‹¤ì–‘í•œ ê±´ë¬¼ ì„¤ê³„ ìš”ì²­",
                    "context": {"stress_test": True}
                },
                expected_output={
                    "success_rate": 0.9,
                    "memory_stable": True
                }
            ),
            TestCase(
                name="error_handling",
                description="ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸",
                input_data={
                    "user_input": "",  # ë¹ˆ ì…ë ¥
                    "context": {"error_test": True}
                },
                expected_output={
                    "graceful_failure": True,
                    "error_message": True
                }
            )
        ]
    
    @performance_monitor("e2e_test_execution")
    async def run_single_test(self, test_case: TestCase) -> Dict[str, Any]:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘: {test_case.name}")
        
        start_time = time.time()
        
        try:
            if test_case.name == "basic_material_recommendation":
                result = await self._test_basic_material_recommendation(test_case)
            elif test_case.name == "complex_building_design":
                result = await self._test_complex_building_design(test_case)
            elif test_case.name == "concurrent_requests":
                result = await self._test_concurrent_requests(test_case)
            elif test_case.name == "stress_test":
                result = await self._test_stress_test(test_case)
            elif test_case.name == "error_handling":
                result = await self._test_error_handling(test_case)
            else:
                result = await self._test_generic(test_case)
            
            execution_time = time.time() - start_time
            
            # ì„±ëŠ¥ ê²€ì¦
            performance_check = self._check_performance_thresholds(execution_time, result)
            
            # ê²°ê³¼ ê²€ì¦
            validation_result = self._validate_test_result(test_case, result)
            
            return {
                "test_name": test_case.name,
                "success": validation_result["success"],
                "execution_time": execution_time,
                "performance_check": performance_check,
                "validation_details": validation_result,
                "raw_result": result
            }
            
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜ ({test_case.name}): {e}")
            return {
                "test_name": test_case.name,
                "success": False,
                "error": str(e),
                "execution_time": time.time() - start_time
            }
    
    async def _test_basic_material_recommendation(self, test_case: TestCase) -> Dict[str, Any]:
        """ê¸°ë³¸ ì¬ë£Œ ì¶”ì²œ í…ŒìŠ¤íŠ¸"""
        input_data = test_case.input_data
        
        result = await self.orchestrator.process_intelligent_request(
            input_data["user_input"],
            context=input_data["context"],
            optimization_level="adaptive"
        )
        
        return result
    
    async def _test_complex_building_design(self, test_case: TestCase) -> Dict[str, Any]:
        """ë³µì¡í•œ ê±´ë¬¼ ì„¤ê³„ í…ŒìŠ¤íŠ¸"""
        input_data = test_case.input_data
        
        # MCP í†µí•© í…ŒìŠ¤íŠ¸ (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.mcp_integration:
            # íŒŒì¼ ì½ê¸° í…ŒìŠ¤íŠ¸
            await self.mcp_integration.request_file_operation("read", "README.md")
        
        result = await self.orchestrator.process_intelligent_request(
            input_data["user_input"],
            context=input_data["context"],
            optimization_level="adaptive"
        )
        
        return result
    
    async def _test_concurrent_requests(self, test_case: TestCase) -> Dict[str, Any]:
        """ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        input_data = test_case.input_data
        concurrent_count = input_data["concurrent_count"]
        
        # ë™ì‹œ ìš”ì²­ ìƒì„±
        tasks = []
        for i in range(concurrent_count):
            task = self.orchestrator.process_intelligent_request(
                f"{input_data['user_input']} #{i+1}",
                context=input_data["context"],
                optimization_level="parallel"
            )
            tasks.append(task)
        
        # ë™ì‹œ ì‹¤í–‰
        start_time = time.time()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        total_time = time.time() - start_time
        
        # ê²°ê³¼ ë¶„ì„
        successful_results = [r for r in results if isinstance(r, dict) and r.get("success", False)]
        success_rate = len(successful_results) / len(results)
        avg_response_time = total_time / concurrent_count
        
        return {
            "concurrent_count": concurrent_count,
            "success_rate": success_rate,
            "avg_response_time": avg_response_time,
            "total_time": total_time,
            "successful_results": len(successful_results),
            "all_success": success_rate == 1.0
        }
    
    async def _test_stress_test(self, test_case: TestCase) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
        input_data = test_case.input_data
        request_count = input_data["request_count"]
        
        results = []
        memory_usage_history = []
        
        for i in range(request_count):
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            import psutil
            memory_usage = psutil.virtual_memory().percent
            memory_usage_history.append(memory_usage)
            
            # ë‹¤ì–‘í•œ ìš”ì²­ ì‹¤í–‰
            test_requests = [
                "ê°„ë‹¨í•œ ì£¼íƒ ì„¤ê³„",
                "ìƒì—…ìš© ê±´ë¬¼ ì¬ë£Œ ì¶”ì²œ",
                "ì¹œí™˜ê²½ ì•„íŒŒíŠ¸ ì„¤ê³„",
                "ì˜¤í”¼ìŠ¤ ë¹Œë”© ì„±ëŠ¥ ë¶„ì„"
            ]
            
            request = test_requests[i % len(test_requests)]
            
            try:
                result = await self.orchestrator.process_intelligent_request(
                    f"{request} (ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ #{i+1})",
                    optimization_level="adaptive"
                )
                results.append(result)
            except Exception as e:
                results.append({"success": False, "error": str(e)})
        
        # ê²°ê³¼ ë¶„ì„
        successful_results = [r for r in results if r.get("success", False)]
        success_rate = len(successful_results) / len(results)
        
        # ë©”ëª¨ë¦¬ ì•ˆì •ì„± í™•ì¸
        memory_stable = max(memory_usage_history) - min(memory_usage_history) < 20  # 20% ì´ë‚´ ë³€ë™
        
        return {
            "request_count": request_count,
            "success_rate": success_rate,
            "memory_stable": memory_stable,
            "memory_usage_range": {
                "min": min(memory_usage_history),
                "max": max(memory_usage_history),
                "avg": sum(memory_usage_history) / len(memory_usage_history)
            }
        }
    
    async def _test_error_handling(self, test_case: TestCase) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        input_data = test_case.input_data
        
        try:
            result = await self.orchestrator.process_intelligent_request(
                input_data["user_input"],
                context=input_data["context"]
            )
            
            # ë¹ˆ ì…ë ¥ì— ëŒ€í•´ graceful failureì¸ì§€ í™•ì¸
            graceful_failure = not result.get("success", True) or "error" in result
            
            return {
                "graceful_failure": graceful_failure,
                "error_message": "error" in result,
                "result": result
            }
            
        except Exception as e:
            # ì˜ˆì™¸ê°€ ë°œìƒí•˜ë©´ graceful failureê°€ ì•„ë‹˜
            return {
                "graceful_failure": False,
                "error_message": True,
                "exception": str(e)
            }
    
    async def _test_generic(self, test_case: TestCase) -> Dict[str, Any]:
        """ì¼ë°˜ì ì¸ í…ŒìŠ¤íŠ¸"""
        input_data = test_case.input_data
        
        result = await self.orchestrator.process_intelligent_request(
            input_data["user_input"],
            context=input_data.get("context", {}),
            optimization_level="adaptive"
        )
        
        return result
    
    def _check_performance_thresholds(self, execution_time: float, result: Dict[str, Any]) -> Dict[str, bool]:
        """ì„±ëŠ¥ ì„ê³„ê°’ í™•ì¸"""
        import psutil
        
        return {
            "response_time_ok": execution_time <= self.performance_thresholds["response_time"],
            "memory_usage_ok": psutil.virtual_memory().percent <= self.performance_thresholds["memory_usage"] / 100 * 100,
            "cpu_usage_ok": psutil.cpu_percent() <= self.performance_thresholds["cpu_usage"]
        }
    
    def _validate_test_result(self, test_case: TestCase, result: Dict[str, Any]) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€ì¦"""
        expected = test_case.expected_output
        validation = {
            "success": True,
            "failed_checks": []
        }
        
        # ì„±ê³µ ì—¬ë¶€ í™•ì¸
        if "success" in expected:
            if result.get("success", False) != expected["success"]:
                validation["success"] = False
                validation["failed_checks"].append(f"Expected success: {expected['success']}, got: {result.get('success', False)}")
        
        # ì¶”ì²œ ì—¬ë¶€ í™•ì¸
        if "has_recommendations" in expected:
            has_recommendations = bool(result.get("recommendations") or result.get("summary", {}).get("total_materials", 0) > 0)
            if has_recommendations != expected["has_recommendations"]:
                validation["success"] = False
                validation["failed_checks"].append(f"Expected recommendations: {expected['has_recommendations']}, got: {has_recommendations}")
        
        # ì‘ë‹µ ì‹œê°„ í™•ì¸
        if "response_time" in expected:
            actual_time = result.get("orchestration_metadata", {}).get("execution_time", 0)
            if actual_time > expected["response_time"]:
                validation["success"] = False
                validation["failed_checks"].append(f"Response time too slow: {actual_time:.2f}s > {expected['response_time']}s")
        
        # ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ ê²€ì¦
        if "all_success" in expected:
            if result.get("all_success", False) != expected["all_success"]:
                validation["success"] = False
                validation["failed_checks"].append(f"Expected all_success: {expected['all_success']}, got: {result.get('all_success', False)}")
        
        # ì„±ê³µë¥  í™•ì¸
        if "success_rate" in expected:
            actual_rate = result.get("success_rate", 0)
            if actual_rate < expected["success_rate"]:
                validation["success"] = False
                validation["failed_checks"].append(f"Success rate too low: {actual_rate:.2f} < {expected['success_rate']:.2f}")
        
        # Graceful failure í™•ì¸
        if "graceful_failure" in expected:
            if result.get("graceful_failure", False) != expected["graceful_failure"]:
                validation["success"] = False
                validation["failed_checks"].append(f"Expected graceful_failure: {expected['graceful_failure']}, got: {result.get('graceful_failure', False)}")
        
        return validation
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("E2E í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹œì‘")
        
        await self.setup()
        
        test_cases = self.define_test_cases()
        results = []
        
        total_start_time = time.time()
        
        for test_case in test_cases:
            logger.info(f"ì‹¤í–‰ ì¤‘: {test_case.name} - {test_case.description}")
            
            # ì¬ì‹œë„ ë¡œì§
            for attempt in range(test_case.retry_count):
                try:
                    result = await asyncio.wait_for(
                        self.run_single_test(test_case),
                        timeout=test_case.timeout
                    )
                    results.append(result)
                    break
                except asyncio.TimeoutError:
                    if attempt == test_case.retry_count - 1:
                        results.append({
                            "test_name": test_case.name,
                            "success": False,
                            "error": "Test timeout",
                            "execution_time": test_case.timeout
                        })
                    else:
                        logger.warning(f"í…ŒìŠ¤íŠ¸ íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{test_case.retry_count})")
                except Exception as e:
                    if attempt == test_case.retry_count - 1:
                        results.append({
                            "test_name": test_case.name,
                            "success": False,
                            "error": str(e),
                            "execution_time": 0
                        })
                    else:
                        logger.warning(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{test_case.retry_count}): {e}")
        
        total_execution_time = time.time() - total_start_time
        
        # ê²°ê³¼ ìš”ì•½
        successful_tests = [r for r in results if r.get("success", False)]
        success_rate = len(successful_tests) / len(results) if results else 0
        avg_execution_time = sum(r.get("execution_time", 0) for r in results) / len(results) if results else 0
        
        # ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        performance_report = await performance_optimizer.generate_optimization_report()
        
        summary = {
            "total_tests": len(results),
            "successful_tests": len(successful_tests),
            "failed_tests": len(results) - len(successful_tests),
            "success_rate": success_rate,
            "total_execution_time": total_execution_time,
            "avg_execution_time_per_test": avg_execution_time,
            "test_results": results,
            "performance_report": performance_report
        }
        
        logger.info(f"E2E í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {len(successful_tests)}/{len(results)} ì„±ê³µ ({success_rate:.1%})")
        
        return summary
    
    async def generate_test_report(self, results: Dict[str, Any]) -> str:
        """í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        report = ["# VIBA AI E2E í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ\n"]
        report.append(f"ì‹¤í–‰ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # ìš”ì•½
        report.append("## ğŸ“Š í…ŒìŠ¤íŠ¸ ìš”ì•½\n")
        report.append(f"- ì´ í…ŒìŠ¤íŠ¸: {results['total_tests']}ê°œ\n")
        report.append(f"- ì„±ê³µ: {results['successful_tests']}ê°œ\n")
        report.append(f"- ì‹¤íŒ¨: {results['failed_tests']}ê°œ\n")
        report.append(f"- ì„±ê³µë¥ : {results['success_rate']:.1%}\n")
        report.append(f"- ì´ ì‹¤í–‰ ì‹œê°„: {results['total_execution_time']:.2f}ì´ˆ\n")
        report.append(f"- í‰ê·  í…ŒìŠ¤íŠ¸ ì‹œê°„: {results['avg_execution_time_per_test']:.2f}ì´ˆ\n\n")
        
        # ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        report.append("## ğŸ“‹ ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼\n")
        for result in results['test_results']:
            status = "âœ… ì„±ê³µ" if result.get("success", False) else "âŒ ì‹¤íŒ¨"
            report.append(f"### {result['test_name']} - {status}\n")
            report.append(f"- ì‹¤í–‰ ì‹œê°„: {result.get('execution_time', 0):.3f}ì´ˆ\n")
            
            if not result.get("success", False):
                report.append(f"- ì˜¤ë¥˜: {result.get('error', 'Unknown error')}\n")
            
            if "validation_details" in result:
                validation = result["validation_details"]
                if not validation["success"] and validation["failed_checks"]:
                    report.append("- ì‹¤íŒ¨í•œ ê²€ì¦:\n")
                    for check in validation["failed_checks"]:
                        report.append(f"  - {check}\n")
            
            report.append("\n")
        
        # ì„±ëŠ¥ ë¶„ì„
        if "performance_report" in results:
            report.append("## ğŸš€ ì„±ëŠ¥ ë¶„ì„\n")
            report.append(results["performance_report"])
        
        # ê¶Œì¥ì‚¬í•­
        report.append("\n## ğŸ’¡ ê¶Œì¥ì‚¬í•­\n")
        if results['success_rate'] < 0.9:
            report.append("- í…ŒìŠ¤íŠ¸ ì„±ê³µë¥ ì´ 90% ë¯¸ë§Œì…ë‹ˆë‹¤. ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ê°œì„ í•˜ì„¸ìš”.\n")
        
        if results['avg_execution_time_per_test'] > 10:
            report.append("- í‰ê·  í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œê°„ì´ 10ì´ˆë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ì„±ëŠ¥ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.\n")
        
        report.append("- ì •ê¸°ì ì¸ E2E í…ŒìŠ¤íŠ¸ ì‹¤í–‰ìœ¼ë¡œ íšŒê·€ ë¬¸ì œë¥¼ ì¡°ê¸°ì— ë°œê²¬í•˜ì„¸ìš”.\n")
        report.append("- CI/CD íŒŒì´í”„ë¼ì¸ì— E2E í…ŒìŠ¤íŠ¸ë¥¼ í†µí•©í•˜ì—¬ ìë™í™”í•˜ì„¸ìš”.\n")
        
        return "".join(report)


async def main():
    """ë©”ì¸ E2E í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ­ Playwright MCP E2E í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ìƒì„±
    test_suite = PlaywrightE2ETestSuite()
    
    try:
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = await test_suite.run_all_tests()
        
        # ë³´ê³ ì„œ ìƒì„±
        report = await test_suite.generate_test_report(results)
        
        # ë³´ê³ ì„œ ì €ì¥
        report_path = "/Users/seunghakwoo/Documents/Cursor/Z/nlp-engine/test_results/e2e_test_report.md"
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   ì„±ê³µë¥ : {results['success_rate']:.1%} ({results['successful_tests']}/{results['total_tests']})")
        print(f"   ì´ ì‹¤í–‰ ì‹œê°„: {results['total_execution_time']:.2f}ì´ˆ")
        print(f"   ë³´ê³ ì„œ ì €ì¥: {report_path}")
        
        if results['success_rate'] >= 0.9:
            print("ğŸ‰ E2E í…ŒìŠ¤íŠ¸ ì„±ê³µ! ì‹œìŠ¤í…œì´ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
        else:
            print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë³´ê³ ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        return results
        
    except Exception as e:
        print(f"âŒ E2E í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    asyncio.run(main())