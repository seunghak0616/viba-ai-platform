#!/usr/bin/env python3
"""
VIBA AI ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
======================

AI ëª¨ë¸ì˜ ì¶”ë¡  ì†ë„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ì •í™•ë„ ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸

@version 1.0
@author VIBA AI Team
@date 2025.07.06
"""

import time
import psutil
import gc
import numpy as np
import json
import logging
from typing import Dict, List, Any, Tuple
from pathlib import Path
import asyncio
from concurrent.futures import ThreadPoolExecutor
import tracemalloc

# í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.ai.viba_core import VIBACoreOrchestrator
from src.utils.logger import setup_logger

logger = setup_logger(__name__)


class ModelPerformanceTester:
    """AI ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.results = {}
        self.start_time = time.time()
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
        self.process = psutil.Process()
        self.initial_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        
        # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
        self.test_scenarios = self._load_test_scenarios()
        
    def _load_test_scenarios(self) -> List[Dict[str, Any]]:
        """í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ"""
        return [
            {
                "name": "ê°„ë‹¨í•œ_ì£¼ê±°_ì„¤ê³„",
                "category": "residential",
                "complexity": "low",
                "input": {
                    "description": "ì„œìš¸ì— 2ì¸µ ë‹¨ë…ì£¼íƒì„ ì„¤ê³„í•´ì£¼ì„¸ìš”",
                    "building_type": "ë‹¨ë…ì£¼íƒ",
                    "style": ["í˜„ëŒ€ì "],
                    "constraints": {
                        "budget": 300000000,
                        "lot_size": 150,
                        "max_floors": 2
                    }
                },
                "expected_duration": 15.0,  # ì´ˆ
                "expected_accuracy": 0.9
            },
            {
                "name": "ë³µì¡í•œ_ìƒì—…_ì„¤ê³„",
                "category": "commercial",
                "complexity": "high", 
                "input": {
                    "description": "ê°•ë‚¨êµ¬ì— 5ì¸µ ìƒì—…ë³µí•©ì‹œì„¤ì„ ì„¤ê³„í•´ì£¼ì„¸ìš”. 1ì¸µì€ ìƒê°€, 2-5ì¸µì€ ì˜¤í”¼ìŠ¤ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”",
                    "building_type": "ìƒì—…ë³µí•©ì‹œì„¤",
                    "style": ["í˜„ëŒ€ì ", "ë¯¸ë‹ˆë©€"],
                    "constraints": {
                        "budget": 2000000000,
                        "lot_size": 500,
                        "max_floors": 5,
                        "parking_spaces": 50
                    }
                },
                "expected_duration": 25.0,
                "expected_accuracy": 0.85
            },
            {
                "name": "ì¹œí™˜ê²½_êµìœ¡ì‹œì„¤",
                "category": "educational",
                "complexity": "medium",
                "input": {
                    "description": "ì¹œí™˜ê²½ ì´ˆë“±í•™êµë¥¼ ì„¤ê³„í•´ì£¼ì„¸ìš”. ìì—°ì±„ê´‘ê³¼ ì—ë„ˆì§€ íš¨ìœ¨ì„ ì¤‘ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”",
                    "building_type": "êµìœ¡ì‹œì„¤",
                    "style": ["ì¹œí™˜ê²½", "ëª¨ë˜"],
                    "constraints": {
                        "budget": 1500000000,
                        "lot_size": 3000,
                        "max_floors": 3,
                        "energy_rating": "A+",
                        "green_certification": True
                    }
                },
                "expected_duration": 30.0,
                "expected_accuracy": 0.88
            },
            {
                "name": "í•œì˜¥_ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤",
                "category": "hospitality",
                "complexity": "medium",
                "input": {
                    "description": "ì „í†µ í•œì˜¥ ìŠ¤íƒ€ì¼ì˜ ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤ë¥¼ ì„¤ê³„í•´ì£¼ì„¸ìš”",
                    "building_type": "ìˆ™ë°•ì‹œì„¤",
                    "style": ["í•œì˜¥", "ì „í†µ"],
                    "constraints": {
                        "budget": 800000000,
                        "lot_size": 400,
                        "max_floors": 2,
                        "rooms": 8
                    }
                },
                "expected_duration": 20.0,
                "expected_accuracy": 0.87
            }
        ]
    
    async def run_single_scenario_test(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info(f"ğŸ”„ í…ŒìŠ¤íŠ¸ ì‹œì‘: {scenario['name']}")
        
        # ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘
        tracemalloc.start()
        start_memory = self.process.memory_info().rss / 1024 / 1024
        start_cpu = self.process.cpu_percent()
        start_time = time.time()
        
        try:
            # VIBA ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            viba = VIBACoreOrchestrator()
            await viba.initialize()
            
            # ì„¤ê³„ ìš”ì²­ ì²˜ë¦¬
            result = await viba.process_design_request(scenario['input'])
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            end_time = time.time()
            duration = end_time - start_time
            
            end_memory = self.process.memory_info().rss / 1024 / 1024
            memory_usage = end_memory - start_memory
            
            cpu_usage = self.process.cpu_percent()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìƒì„¸ ë¶„ì„
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            
            # ì •í™•ë„ í‰ê°€
            accuracy_score = result.get('quality_score', 0.0)
            
            # ê²°ê³¼ ì •ë¦¬
            test_result = {
                "scenario_name": scenario['name'],
                "category": scenario['category'],
                "complexity": scenario['complexity'],
                "status": result.get('status', 'unknown'),
                "performance_metrics": {
                    "duration": duration,
                    "expected_duration": scenario['expected_duration'],
                    "duration_ratio": duration / scenario['expected_duration'],
                    "memory_usage_mb": memory_usage,
                    "peak_memory_mb": peak / 1024 / 1024,
                    "cpu_usage_percent": cpu_usage,
                    "accuracy_score": accuracy_score,
                    "expected_accuracy": scenario['expected_accuracy'],
                    "accuracy_ratio": accuracy_score / scenario['expected_accuracy']
                },
                "quality_metrics": {
                    "bim_generation_success": result.get('result', {}).get('bim_model') is not None,
                    "design_concept_quality": len(result.get('result', {}).get('design_concept', '')),
                    "performance_analysis_complete": result.get('result', {}).get('performance_report') is not None
                },
                "resource_efficiency": {
                    "memory_per_complexity": memory_usage / {"low": 1, "medium": 2, "high": 3}[scenario['complexity']],
                    "time_per_complexity": duration / {"low": 1, "medium": 2, "high": 3}[scenario['complexity']],
                    "meets_performance_target": duration <= scenario['expected_duration'] * 1.1,  # 10% ì—¬ìœ 
                    "meets_accuracy_target": accuracy_score >= scenario['expected_accuracy'] * 0.9  # 10% ì—¬ìœ 
                }
            }
            
            # VIBA ì‹œìŠ¤í…œ ì •ë¦¬
            await viba.shutdown()
            
            logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {scenario['name']} ({duration:.2f}ì´ˆ, ì •í™•ë„: {accuracy_score:.3f})")
            
            return test_result
            
        except Exception as e:
            logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {scenario['name']} - {e}")
            tracemalloc.stop()
            
            return {
                "scenario_name": scenario['name'],
                "category": scenario['category'],
                "status": "failed",
                "error": str(e),
                "performance_metrics": {
                    "duration": time.time() - start_time,
                    "memory_usage_mb": (self.process.memory_info().rss / 1024 / 1024) - start_memory,
                    "accuracy_score": 0.0
                }
            }
    
    async def run_concurrent_load_test(self, concurrent_users: int = 5) -> Dict[str, Any]:
        """ë™ì‹œ ì‚¬ìš©ì ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        logger.info(f"ğŸš€ ë™ì‹œ ì‚¬ìš©ì ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œì‘: {concurrent_users}ëª…")
        
        start_time = time.time()
        start_memory = self.process.memory_info().rss / 1024 / 1024
        
        # ë™ì‹œ ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤ ì¤€ë¹„
        concurrent_scenarios = []
        for i in range(concurrent_users):
            scenario = self.test_scenarios[i % len(self.test_scenarios)].copy()
            scenario['user_id'] = f"user_{i+1}"
            concurrent_scenarios.append(scenario)
        
        # ë™ì‹œ ì‹¤í–‰
        tasks = [self.run_single_scenario_test(scenario) for scenario in concurrent_scenarios]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ë¶„ì„
        end_time = time.time()
        total_duration = end_time - start_time
        end_memory = self.process.memory_info().rss / 1024 / 1024
        total_memory_usage = end_memory - start_memory
        
        successful_results = [r for r in results if isinstance(r, dict) and r.get('status') != 'failed']
        failed_results = [r for r in results if isinstance(r, Exception) or (isinstance(r, dict) and r.get('status') == 'failed')]
        
        # ì„±ëŠ¥ í†µê³„
        if successful_results:
            durations = [r['performance_metrics']['duration'] for r in successful_results]
            accuracies = [r['performance_metrics']['accuracy_score'] for r in successful_results]
            
            performance_stats = {
                "total_requests": concurrent_users,
                "successful_requests": len(successful_results),
                "failed_requests": len(failed_results),
                "success_rate": len(successful_results) / concurrent_users,
                "total_duration": total_duration,
                "average_response_time": np.mean(durations),
                "p95_response_time": np.percentile(durations, 95),
                "max_response_time": np.max(durations),
                "min_response_time": np.min(durations),
                "average_accuracy": np.mean(accuracies),
                "throughput_requests_per_second": concurrent_users / total_duration,
                "total_memory_usage_mb": total_memory_usage,
                "memory_per_request_mb": total_memory_usage / concurrent_users
            }
        else:
            performance_stats = {
                "total_requests": concurrent_users,
                "successful_requests": 0,
                "failed_requests": concurrent_users,
                "success_rate": 0.0,
                "total_duration": total_duration
            }
        
        return {
            "test_type": "concurrent_load",
            "concurrent_users": concurrent_users,
            "performance_stats": performance_stats,
            "individual_results": results,
            "system_impact": {
                "peak_memory_usage_mb": end_memory,
                "memory_increase_mb": total_memory_usage,
                "cpu_utilization": psutil.cpu_percent(interval=1)
            }
        }
    
    async def run_stress_test(self, max_concurrent: int = 20, step_size: int = 5) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ - ì ì§„ì  ë¶€í•˜ ì¦ê°€"""
        logger.info(f"âš¡ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘: ìµœëŒ€ {max_concurrent}ëª…")
        
        stress_results = []
        current_concurrent = step_size
        
        while current_concurrent <= max_concurrent:
            logger.info(f"ğŸ“Š ë¶€í•˜ ë ˆë²¨: {current_concurrent}ëª… ë™ì‹œ ì‚¬ìš©ì")
            
            # í˜„ì¬ ë¶€í•˜ ë ˆë²¨ì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            load_result = await self.run_concurrent_load_test(current_concurrent)
            load_result['load_level'] = current_concurrent
            stress_results.append(load_result)
            
            # ì„±ëŠ¥ ì €í•˜ í™•ì¸
            success_rate = load_result['performance_stats']['success_rate']
            avg_response_time = load_result['performance_stats'].get('average_response_time', float('inf'))
            
            logger.info(f"ë¶€í•˜ ë ˆë²¨ {current_concurrent}: ì„±ê³µë¥  {success_rate:.2%}, í‰ê·  ì‘ë‹µì‹œê°„ {avg_response_time:.2f}ì´ˆ")
            
            # ì„ê³„ì  ë„ë‹¬ ì‹œ ì¤‘ë‹¨
            if success_rate < 0.8 or avg_response_time > 60:
                logger.warning(f"ì„±ëŠ¥ ì„ê³„ì  ë„ë‹¬. í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨: {current_concurrent}ëª…")
                break
            
            current_concurrent += step_size
            
            # ì‹œìŠ¤í…œ ë³µêµ¬ ëŒ€ê¸°
            await asyncio.sleep(5)
            gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
        max_stable_load = max([r['load_level'] for r in stress_results if r['performance_stats']['success_rate'] >= 0.95])
        breaking_point = min([r['load_level'] for r in stress_results if r['performance_stats']['success_rate'] < 0.8], default=max_concurrent)
        
        return {
            "test_type": "stress_test",
            "max_tested_load": max([r['load_level'] for r in stress_results]),
            "max_stable_load": max_stable_load,
            "breaking_point": breaking_point,
            "load_test_results": stress_results,
            "performance_degradation": self._analyze_performance_degradation(stress_results)
        }
    
    def _analyze_performance_degradation(self, stress_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì €í•˜ íŒ¨í„´ ë¶„ì„"""
        if len(stress_results) < 2:
            return {}
        
        baseline = stress_results[0]['performance_stats']
        final = stress_results[-1]['performance_stats']
        
        return {
            "response_time_degradation": final.get('average_response_time', 0) / baseline.get('average_response_time', 1),
            "success_rate_degradation": baseline.get('success_rate', 1) - final.get('success_rate', 0),
            "memory_growth_rate": final.get('total_memory_usage_mb', 0) / final.get('total_requests', 1),
            "throughput_degradation": baseline.get('throughput_requests_per_second', 1) / final.get('throughput_requests_per_second', 1)
        }
    
    async def run_endurance_test(self, duration_minutes: int = 30) -> Dict[str, Any]:
        """ì§€ì†ì„± í…ŒìŠ¤íŠ¸ - ì¥ì‹œê°„ ì—°ì† ì‹¤í–‰"""
        logger.info(f"ğŸ•’ ì§€ì†ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘: {duration_minutes}ë¶„")
        
        start_time = time.time()
        end_time = start_time + (duration_minutes * 60)
        
        test_results = []
        iteration = 0
        
        while time.time() < end_time:
            iteration += 1
            logger.info(f"ì§€ì†ì„± í…ŒìŠ¤íŠ¸ ë°˜ë³µ {iteration}")
            
            # ëœë¤ ì‹œë‚˜ë¦¬ì˜¤ ì„ íƒ
            scenario = np.random.choice(self.test_scenarios)
            result = await self.run_single_scenario_test(scenario)
            result['iteration'] = iteration
            result['elapsed_time'] = time.time() - start_time
            
            test_results.append(result)
            
            # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì²´í¬
            current_memory = self.process.memory_info().rss / 1024 / 1024
            if current_memory > self.initial_memory * 2:  # ë©”ëª¨ë¦¬ 2ë°° ì¦ê°€ ì‹œ ê²½ê³ 
                logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸‰ì¦ ê°ì§€: {current_memory:.1f}MB")
            
            # ì§§ì€ ëŒ€ê¸°
            await asyncio.sleep(10)
        
        # ì§€ì†ì„± í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
        successful_tests = [r for r in test_results if r.get('status') != 'failed']
        
        if successful_tests:
            durations = [r['performance_metrics']['duration'] for r in successful_tests]
            accuracies = [r['performance_metrics']['accuracy_score'] for r in successful_tests]
            memory_usages = [r['performance_metrics']['memory_usage_mb'] for r in successful_tests]
            
            endurance_stats = {
                "test_duration_minutes": duration_minutes,
                "total_iterations": len(test_results),
                "successful_iterations": len(successful_tests),
                "failure_rate": (len(test_results) - len(successful_tests)) / len(test_results),
                "average_response_time": np.mean(durations),
                "response_time_stability": np.std(durations),
                "average_accuracy": np.mean(accuracies),
                "accuracy_stability": np.std(accuracies),
                "memory_growth_trend": np.polyfit(range(len(memory_usages)), memory_usages, 1)[0],
                "peak_memory_usage": max(memory_usages),
                "memory_leak_detected": np.polyfit(range(len(memory_usages)), memory_usages, 1)[0] > 1.0  # 1MB/iteration ì¦ê°€ ì‹œ ëˆ„ìˆ˜ ì˜ì‹¬
            }
        else:
            endurance_stats = {
                "test_duration_minutes": duration_minutes,
                "total_iterations": len(test_results),
                "successful_iterations": 0,
                "failure_rate": 1.0
            }
        
        return {
            "test_type": "endurance_test",
            "endurance_stats": endurance_stats,
            "iteration_results": test_results,
            "system_stability": {
                "final_memory_usage_mb": self.process.memory_info().rss / 1024 / 1024,
                "memory_increase_total_mb": (self.process.memory_info().rss / 1024 / 1024) - self.initial_memory,
                "cpu_average": psutil.cpu_percent(interval=1)
            }
        }
    
    async def run_comprehensive_performance_test(self) -> Dict[str, Any]:
        """ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ VIBA AI ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        comprehensive_results = {
            "test_session": {
                "start_time": time.time(),
                "system_info": {
                    "cpu_count": psutil.cpu_count(),
                    "memory_total_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024,
                    "python_version": sys.version,
                    "platform": sys.platform
                }
            }
        }
        
        # 1. ê°œë³„ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ“‹ 1. ê°œë³„ ì‹œë‚˜ë¦¬ì˜¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        scenario_results = []
        for scenario in self.test_scenarios:
            result = await self.run_single_scenario_test(scenario)
            scenario_results.append(result)
            await asyncio.sleep(2)  # í…ŒìŠ¤íŠ¸ ê°„ ê°„ê²©
        
        comprehensive_results["scenario_tests"] = scenario_results
        
        # 2. ë™ì‹œ ì‚¬ìš©ì í…ŒìŠ¤íŠ¸
        logger.info("ğŸ‘¥ 2. ë™ì‹œ ì‚¬ìš©ì ë¶€í•˜ í…ŒìŠ¤íŠ¸")
        concurrent_result = await self.run_concurrent_load_test(5)
        comprehensive_results["concurrent_load_test"] = concurrent_result
        
        # 3. ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
        logger.info("âš¡ 3. ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸")
        stress_result = await self.run_stress_test(15, 3)
        comprehensive_results["stress_test"] = stress_result
        
        # 4. ì§€ì†ì„± í…ŒìŠ¤íŠ¸ (ë‹¨ì¶•ëœ ë²„ì „)
        logger.info("ğŸ•’ 4. ì§€ì†ì„± í…ŒìŠ¤íŠ¸")
        endurance_result = await self.run_endurance_test(10)  # 10ë¶„ìœ¼ë¡œ ë‹¨ì¶•
        comprehensive_results["endurance_test"] = endurance_result
        
        # ì¢…í•© í‰ê°€
        comprehensive_results["overall_assessment"] = self._generate_overall_assessment(comprehensive_results)
        comprehensive_results["test_session"]["end_time"] = time.time()
        comprehensive_results["test_session"]["total_duration"] = time.time() - comprehensive_results["test_session"]["start_time"]
        
        return comprehensive_results
    
    def _generate_overall_assessment(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢…í•© í‰ê°€ ìƒì„±"""
        scenario_results = results.get("scenario_tests", [])
        concurrent_result = results.get("concurrent_load_test", {})
        stress_result = results.get("stress_test", {})
        endurance_result = results.get("endurance_test", {})
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°
        performance_scores = []
        
        # ê°œë³„ ì‹œë‚˜ë¦¬ì˜¤ ì„±ëŠ¥
        if scenario_results:
            scenario_success_rate = len([r for r in scenario_results if r.get('status') != 'failed']) / len(scenario_results)
            avg_accuracy = np.mean([r['performance_metrics']['accuracy_score'] for r in scenario_results if r.get('status') != 'failed'])
            performance_scores.append(scenario_success_rate * avg_accuracy)
        
        # ë™ì‹œ ì²˜ë¦¬ ì„±ëŠ¥
        if concurrent_result.get("performance_stats"):
            concurrent_score = concurrent_result["performance_stats"]["success_rate"]
            performance_scores.append(concurrent_score)
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ ë‚´ì„±
        if stress_result.get("max_stable_load"):
            stress_score = min(stress_result["max_stable_load"] / 10, 1.0)  # 10ëª…ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
            performance_scores.append(stress_score)
        
        # ì•ˆì •ì„±
        if endurance_result.get("endurance_stats"):
            stability_score = 1 - endurance_result["endurance_stats"]["failure_rate"]
            performance_scores.append(stability_score)
        
        overall_score = np.mean(performance_scores) if performance_scores else 0.0
        
        # ë“±ê¸‰ ë¶€ì—¬
        if overall_score >= 0.9:
            grade = "A"
            assessment = "Excellent"
        elif overall_score >= 0.8:
            grade = "B"
            assessment = "Good"
        elif overall_score >= 0.7:
            grade = "C"
            assessment = "Acceptable"
        elif overall_score >= 0.6:
            grade = "D"
            assessment = "Poor"
        else:
            grade = "F"
            assessment = "Unacceptable"
        
        return {
            "overall_score": overall_score,
            "performance_grade": grade,
            "assessment": assessment,
            "component_scores": {
                "scenario_performance": performance_scores[0] if len(performance_scores) > 0 else 0,
                "concurrent_handling": performance_scores[1] if len(performance_scores) > 1 else 0,
                "stress_tolerance": performance_scores[2] if len(performance_scores) > 2 else 0,
                "system_stability": performance_scores[3] if len(performance_scores) > 3 else 0
            },
            "recommendations": self._generate_performance_recommendations(results)
        }
    
    def _generate_performance_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ê°œë³„ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
        scenario_results = results.get("scenario_tests", [])
        slow_scenarios = [r for r in scenario_results if r.get('performance_metrics', {}).get('duration_ratio', 0) > 1.2]
        if slow_scenarios:
            recommendations.append(f"{len(slow_scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ì˜ ì‘ë‹µì‹œê°„ì´ ëª©í‘œë¥¼ 20% ì´ˆê³¼í•©ë‹ˆë‹¤. ì•Œê³ ë¦¬ì¦˜ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        high_memory_scenarios = [r for r in scenario_results if r.get('performance_metrics', {}).get('memory_usage_mb', 0) > 500]
        if high_memory_scenarios:
            recommendations.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ì€ ì‹œë‚˜ë¦¬ì˜¤ê°€ ìˆìŠµë‹ˆë‹¤. ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ì •í™•ë„ ë¶„ì„
        low_accuracy_scenarios = [r for r in scenario_results if r.get('performance_metrics', {}).get('accuracy_ratio', 0) < 0.9]
        if low_accuracy_scenarios:
            recommendations.append("ì¼ë¶€ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì •í™•ë„ê°€ ëª©í‘œì— ë¯¸ë‹¬í•©ë‹ˆë‹¤. ëª¨ë¸ ì¬í›ˆë ¨ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ë™ì‹œ ì²˜ë¦¬ ì„±ëŠ¥
        concurrent_result = results.get("concurrent_load_test", {})
        if concurrent_result.get("performance_stats", {}).get("success_rate", 1) < 0.95:
            recommendations.append("ë™ì‹œ ì‚¬ìš©ì ì²˜ë¦¬ ì„±ëŠ¥ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        stress_result = results.get("stress_test", {})
        if stress_result.get("max_stable_load", 0) < 10:
            recommendations.append("ìŠ¤íŠ¸ë ˆìŠ¤ ë‚´ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œ í™•ì¥ì„± ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return recommendations


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸ¤– VIBA AI ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    tester = ModelPerformanceTester()
    
    try:
        # ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = await tester.run_comprehensive_performance_test()
        
        # ê²°ê³¼ ì €ì¥
        results_file = Path(__file__).parent.parent.parent / "test-results" / f"model_performance_{int(time.time())}.json"
        results_file.parent.mkdir(exist_ok=True)
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ¤– VIBA AI ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("="*60)
        
        assessment = results.get("overall_assessment", {})
        print(f"ì „ì²´ ì ìˆ˜: {assessment.get('overall_score', 0):.3f}")
        print(f"ì„±ëŠ¥ ë“±ê¸‰: {assessment.get('performance_grade', 'N/A')}")
        print(f"í‰ê°€: {assessment.get('assessment', 'Unknown')}")
        
        print("\nì»´í¬ë„ŒíŠ¸ë³„ ì ìˆ˜:")
        component_scores = assessment.get("component_scores", {})
        for component, score in component_scores.items():
            print(f"  - {component}: {score:.3f}")
        
        print("\nê°œì„  ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(assessment.get("recommendations", []), 1):
            print(f"  {i}. {rec}")
        
        print(f"\nìƒì„¸ ê²°ê³¼: {results_file}")
        
        # ì„±ëŠ¥ ê¸°ì¤€ ë¯¸ë‹¬ ì‹œ ì¢…ë£Œ ì½”ë“œ 1
        if assessment.get('overall_score', 0) < 0.7:
            logger.warning("ì„±ëŠ¥ ê¸°ì¤€ ë¯¸ë‹¬!")
            sys.exit(1)
        
    except Exception as e:
        logger.error(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())