"""
VIBA AI ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ëª¨ë“ˆ
==============================

ì‹œìŠ¤í…œ ì „ì²´ì˜ ì„±ëŠ¥ì„ ë¶„ì„í•˜ê³  ìµœì í™”í•˜ëŠ” ê³ ê¸‰ ëª¨ë“ˆ

@version 1.0
@author VIBA AI Team
@date 2025.07.06
"""

import asyncio
import time
import cProfile
import pstats
import io
import psutil
import json
import logging
import os
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from collections import defaultdict, deque
import numpy as np
from functools import lru_cache, wraps
import concurrent.futures
import aiofiles
import pickle

logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    function_name: str
    execution_time: float
    memory_usage: float
    cpu_usage: float
    call_count: int = 1
    cache_hits: int = 0
    cache_misses: int = 0
    
    @property
    def avg_execution_time(self) -> float:
        return self.execution_time / self.call_count if self.call_count > 0 else 0
    
    @property
    def cache_hit_rate(self) -> float:
        total_cache_calls = self.cache_hits + self.cache_misses
        return self.cache_hits / total_cache_calls if total_cache_calls > 0 else 0


class PerformanceOptimizer:
    """ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™”ê¸°"""
    
    def __init__(self):
        self.metrics = defaultdict(lambda: PerformanceMetrics("", 0, 0, 0))
        self.profiler = cProfile.Profile()
        self.optimization_history = deque(maxlen=1000)
        
        # ìºì‹± ì„¤ì •
        self.cache_config = {
            "max_size": 1000,
            "ttl": 300,  # 5ë¶„
            "strategy": "lru"
        }
        
        # ë¹„ë™ê¸° ì²˜ë¦¬ ì„¤ì •
        self.async_config = {
            "max_workers": 10,
            "queue_size": 100,
            "timeout": 30
        }
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •
        self.memory_config = {
            "max_memory_percent": 80,
            "gc_threshold": 70,
            "cache_cleanup_threshold": 60
        }
    
    def performance_monitor(self, name: Optional[str] = None):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
        def decorator(func: Callable):
            @wraps(func)
            async def async_wrapper(*args, **kwargs):
                func_name = name or f"{func.__module__}.{func.__name__}"
                
                # ì‹œì‘ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                start_time = time.time()
                start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
                start_cpu = psutil.cpu_percent(interval=None)
                
                try:
                    # í•¨ìˆ˜ ì‹¤í–‰
                    result = await func(*args, **kwargs)
                    
                    # ì¢…ë£Œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                    end_time = time.time()
                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
                    end_cpu = psutil.cpu_percent(interval=None)
                    
                    # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                    metrics = self.metrics[func_name]
                    metrics.function_name = func_name
                    metrics.execution_time += (end_time - start_time)
                    metrics.memory_usage = max(metrics.memory_usage, end_memory - start_memory)
                    metrics.cpu_usage = max(metrics.cpu_usage, end_cpu - start_cpu)
                    metrics.call_count += 1
                    
                    # ì„±ëŠ¥ ì´ìƒ ê°ì§€
                    if end_time - start_time > 1.0:  # 1ì´ˆ ì´ìƒ
                        logger.warning(f"Slow function: {func_name} took {end_time - start_time:.3f}s")
                    
                    return result
                    
                except Exception as e:
                    logger.error(f"Error in {func_name}: {e}")
                    raise
            
            @wraps(func)
            def sync_wrapper(*args, **kwargs):
                func_name = name or f"{func.__module__}.{func.__name__}"
                
                start_time = time.time()
                start_memory = psutil.Process().memory_info().rss / 1024 / 1024
                start_cpu = psutil.cpu_percent(interval=None)
                
                try:
                    result = func(*args, **kwargs)
                    
                    end_time = time.time()
                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    end_cpu = psutil.cpu_percent(interval=None)
                    
                    metrics = self.metrics[func_name]
                    metrics.function_name = func_name
                    metrics.execution_time += (end_time - start_time)
                    metrics.memory_usage = max(metrics.memory_usage, end_memory - start_memory)
                    metrics.cpu_usage = max(metrics.cpu_usage, end_cpu - start_cpu)
                    metrics.call_count += 1
                    
                    return result
                    
                except Exception as e:
                    logger.error(f"Error in {func_name}: {e}")
                    raise
            
            return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
        
        return decorator
    
    def optimized_cache(self, maxsize: int = 128, ttl: int = 300):
        """ìµœì í™”ëœ ìºì‹± ë°ì½”ë ˆì´í„°"""
        def decorator(func: Callable):
            cache = {}
            cache_times = {}
            
            @wraps(func)
            async def async_wrapper(*args, **kwargs):
                # ìºì‹œ í‚¤ ìƒì„±
                cache_key = str(args) + str(kwargs)
                
                # ìºì‹œ í™•ì¸
                if cache_key in cache:
                    # TTL í™•ì¸
                    if time.time() - cache_times[cache_key] < ttl:
                        self.metrics[func.__name__].cache_hits += 1
                        return cache[cache_key]
                    else:
                        # ë§Œë£Œëœ ìºì‹œ ì œê±°
                        del cache[cache_key]
                        del cache_times[cache_key]
                
                # ìºì‹œ ë¯¸ìŠ¤
                self.metrics[func.__name__].cache_misses += 1
                
                # í•¨ìˆ˜ ì‹¤í–‰
                result = await func(*args, **kwargs)
                
                # ìºì‹œ í¬ê¸° ê´€ë¦¬
                if len(cache) >= maxsize:
                    # LRU ë°©ì‹ìœ¼ë¡œ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                    oldest_key = min(cache_times, key=cache_times.get)
                    del cache[oldest_key]
                    del cache_times[oldest_key]
                
                # ìºì‹œ ì €ì¥
                cache[cache_key] = result
                cache_times[cache_key] = time.time()
                
                return result
            
            @wraps(func)
            def sync_wrapper(*args, **kwargs):
                cache_key = str(args) + str(kwargs)
                
                if cache_key in cache:
                    if time.time() - cache_times[cache_key] < ttl:
                        self.metrics[func.__name__].cache_hits += 1
                        return cache[cache_key]
                    else:
                        del cache[cache_key]
                        del cache_times[cache_key]
                
                self.metrics[func.__name__].cache_misses += 1
                result = func(*args, **kwargs)
                
                if len(cache) >= maxsize:
                    oldest_key = min(cache_times, key=cache_times.get)
                    del cache[oldest_key]
                    del cache_times[oldest_key]
                
                cache[cache_key] = result
                cache_times[cache_key] = time.time()
                
                return result
            
            return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
        
        return decorator
    
    async def profile_async_function(self, func: Callable, *args, **kwargs):
        """ë¹„ë™ê¸° í•¨ìˆ˜ í”„ë¡œíŒŒì¼ë§"""
        # CPU í”„ë¡œíŒŒì¼ë§
        self.profiler.enable()
        
        try:
            result = await func(*args, **kwargs)
        finally:
            self.profiler.disable()
        
        # í”„ë¡œíŒŒì¼ ê²°ê³¼ ë¶„ì„
        s = io.StringIO()
        ps = pstats.Stats(self.profiler, stream=s).sort_stats('cumulative')
        ps.print_stats(20)  # ìƒìœ„ 20ê°œ í•¨ìˆ˜
        
        profile_result = s.getvalue()
        
        return result, profile_result
    
    async def optimize_database_queries(self, queries: List[str]) -> List[str]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”"""
        optimized_queries = []
        
        for query in queries:
            # ì¿¼ë¦¬ ë¶„ì„
            if "SELECT" in query.upper():
                # ì¸ë±ìŠ¤ íŒíŠ¸ ì¶”ê°€
                if "WHERE" in query.upper() and "/*+ INDEX" not in query:
                    query = query.replace("SELECT", "SELECT /*+ INDEX */", 1)
                
                # LIMIT ì¶”ê°€ (ì—†ëŠ” ê²½ìš°)
                if "LIMIT" not in query.upper():
                    query += " LIMIT 1000"
            
            # ì¡°ì¸ ìµœì í™”
            if "JOIN" in query.upper():
                # INNER JOINì„ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©
                query = query.replace("LEFT JOIN", "LEFT OUTER JOIN")
            
            optimized_queries.append(query)
        
        return optimized_queries
    
    async def optimize_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        current_memory = psutil.virtual_memory().percent
        
        if current_memory > self.memory_config["max_memory_percent"]:
            logger.warning(f"High memory usage: {current_memory}%")
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            import gc
            gc.collect()
            
            # ìºì‹œ ì •ë¦¬
            if current_memory > self.memory_config["cache_cleanup_threshold"]:
                await self._cleanup_caches()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¬í™•ì¸
            new_memory = psutil.virtual_memory().percent
            logger.info(f"Memory optimized: {current_memory}% -> {new_memory}%")
    
    async def _cleanup_caches(self):
        """ìºì‹œ ì •ë¦¬"""
        # LRU ìºì‹œ ì •ë¦¬
        for attr_name in dir(self):
            attr = getattr(self, attr_name)
            if hasattr(attr, 'cache_clear'):
                attr.cache_clear()
                logger.info(f"Cleared cache for {attr_name}")
    
    async def optimize_async_operations(self, operations: List[Callable]) -> List[Any]:
        """ë¹„ë™ê¸° ì‘ì—… ìµœì í™”"""
        # ì‘ì—…ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
        batch_size = self.async_config["max_workers"]
        results = []
        
        for i in range(0, len(operations), batch_size):
            batch = operations[i:i + batch_size]
            
            # ë°°ì¹˜ ë³‘ë ¬ ì‹¤í–‰
            batch_results = await asyncio.gather(
                *[op() for op in batch],
                return_exceptions=True
            )
            
            results.extend(batch_results)
        
        return results
    
    def parallel_process(self, func: Callable, items: List[Any], max_workers: int = None) -> List[Any]:
        """ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”"""
        max_workers = max_workers or self.async_config["max_workers"]
        
        with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
            results = list(executor.map(func, items))
        
        return results
    
    async def analyze_bottlenecks(self) -> Dict[str, Any]:
        """ë³‘ëª©ì  ë¶„ì„"""
        bottlenecks = {
            "slow_functions": [],
            "memory_intensive": [],
            "cpu_intensive": [],
            "cache_inefficient": []
        }
        
        for func_name, metrics in self.metrics.items():
            # ëŠë¦° í•¨ìˆ˜ (í‰ê·  ì‹¤í–‰ ì‹œê°„ > 0.5ì´ˆ)
            if metrics.avg_execution_time > 0.5:
                bottlenecks["slow_functions"].append({
                    "function": func_name,
                    "avg_time": metrics.avg_execution_time,
                    "call_count": metrics.call_count
                })
            
            # ë©”ëª¨ë¦¬ ì§‘ì•½ì  í•¨ìˆ˜ (> 100MB)
            if metrics.memory_usage > 100:
                bottlenecks["memory_intensive"].append({
                    "function": func_name,
                    "memory_mb": metrics.memory_usage
                })
            
            # CPU ì§‘ì•½ì  í•¨ìˆ˜ (> 50%)
            if metrics.cpu_usage > 50:
                bottlenecks["cpu_intensive"].append({
                    "function": func_name,
                    "cpu_percent": metrics.cpu_usage
                })
            
            # ìºì‹œ ë¹„íš¨ìœ¨ì  í•¨ìˆ˜ (ìºì‹œ íˆíŠ¸ìœ¨ < 50%)
            if metrics.cache_hit_rate < 0.5 and (metrics.cache_hits + metrics.cache_misses) > 10:
                bottlenecks["cache_inefficient"].append({
                    "function": func_name,
                    "hit_rate": metrics.cache_hit_rate,
                    "total_calls": metrics.cache_hits + metrics.cache_misses
                })
        
        # ì •ë ¬
        for category in bottlenecks:
            if category == "slow_functions":
                bottlenecks[category].sort(key=lambda x: x["avg_time"], reverse=True)
            elif category == "memory_intensive":
                bottlenecks[category].sort(key=lambda x: x["memory_mb"], reverse=True)
            elif category == "cpu_intensive":
                bottlenecks[category].sort(key=lambda x: x["cpu_percent"], reverse=True)
            elif category == "cache_inefficient":
                bottlenecks[category].sort(key=lambda x: x["hit_rate"])
        
        return bottlenecks
    
    async def generate_optimization_report(self) -> str:
        """ìµœì í™” ë³´ê³ ì„œ ìƒì„±"""
        bottlenecks = await self.analyze_bottlenecks()
        
        report = ["# VIBA AI ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ë³´ê³ ì„œ\n"]
        report.append(f"ìƒì„± ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ì „ì²´ ìš”ì•½
        total_functions = len(self.metrics)
        total_calls = sum(m.call_count for m in self.metrics.values())
        total_time = sum(m.execution_time for m in self.metrics.values())
        
        report.append("## ì „ì²´ ìš”ì•½\n")
        report.append(f"- ëª¨ë‹ˆí„°ë§ëœ í•¨ìˆ˜: {total_functions}ê°œ\n")
        report.append(f"- ì´ í˜¸ì¶œ íšŸìˆ˜: {total_calls:,}íšŒ\n")
        report.append(f"- ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ\n")
        avg_time = total_time/total_calls if total_calls > 0 else 0
        report.append(f"- í‰ê·  í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„: {avg_time:.4f}ì´ˆ\n\n")
        
        # ë³‘ëª©ì  ë¶„ì„
        report.append("## ë³‘ëª©ì  ë¶„ì„\n")
        
        # ëŠë¦° í•¨ìˆ˜
        if bottlenecks["slow_functions"]:
            report.append("### ğŸŒ ëŠë¦° í•¨ìˆ˜ (ìƒìœ„ 5ê°œ)\n")
            for i, func in enumerate(bottlenecks["slow_functions"][:5]):
                report.append(f"{i+1}. **{func['function']}**\n")
                report.append(f"   - í‰ê·  ì‹¤í–‰ ì‹œê°„: {func['avg_time']:.3f}ì´ˆ\n")
                report.append(f"   - í˜¸ì¶œ íšŸìˆ˜: {func['call_count']}íšŒ\n")
        
        # ë©”ëª¨ë¦¬ ì§‘ì•½ì  í•¨ìˆ˜
        if bottlenecks["memory_intensive"]:
            report.append("\n### ğŸ’¾ ë©”ëª¨ë¦¬ ì§‘ì•½ì  í•¨ìˆ˜ (ìƒìœ„ 5ê°œ)\n")
            for i, func in enumerate(bottlenecks["memory_intensive"][:5]):
                report.append(f"{i+1}. **{func['function']}**\n")
                report.append(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {func['memory_mb']:.1f}MB\n")
        
        # CPU ì§‘ì•½ì  í•¨ìˆ˜
        if bottlenecks["cpu_intensive"]:
            report.append("\n### ğŸ”¥ CPU ì§‘ì•½ì  í•¨ìˆ˜ (ìƒìœ„ 5ê°œ)\n")
            for i, func in enumerate(bottlenecks["cpu_intensive"][:5]):
                report.append(f"{i+1}. **{func['function']}**\n")
                report.append(f"   - CPU ì‚¬ìš©ë¥ : {func['cpu_percent']:.1f}%\n")
        
        # ìºì‹œ ë¹„íš¨ìœ¨ì  í•¨ìˆ˜
        if bottlenecks["cache_inefficient"]:
            report.append("\n### ğŸ“¦ ìºì‹œ ë¹„íš¨ìœ¨ì  í•¨ìˆ˜ (ìƒìœ„ 5ê°œ)\n")
            for i, func in enumerate(bottlenecks["cache_inefficient"][:5]):
                report.append(f"{i+1}. **{func['function']}**\n")
                report.append(f"   - ìºì‹œ íˆíŠ¸ìœ¨: {func['hit_rate']:.1%}\n")
                report.append(f"   - ì´ ìºì‹œ ì ‘ê·¼: {func['total_calls']}íšŒ\n")
        
        # ìµœì í™” ê¶Œì¥ì‚¬í•­
        report.append("\n## ğŸ’¡ ìµœì í™” ê¶Œì¥ì‚¬í•­\n")
        
        recommendations = await self._generate_recommendations(bottlenecks)
        for i, rec in enumerate(recommendations):
            report.append(f"{i+1}. {rec}\n")
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í˜„í™©
        report.append("\n## ğŸ“Š ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í˜„í™©\n")
        report.append(f"- CPU ì‚¬ìš©ë¥ : {psutil.cpu_percent()}%\n")
        report.append(f"- ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {psutil.virtual_memory().percent}%\n")
        report.append(f"- ë””ìŠ¤í¬ ì‚¬ìš©ë¥ : {psutil.disk_usage('/').percent}%\n")
        
        return "".join(report)
    
    async def _generate_recommendations(self, bottlenecks: Dict[str, List]) -> List[str]:
        """ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ëŠë¦° í•¨ìˆ˜ì— ëŒ€í•œ ê¶Œì¥ì‚¬í•­
        if bottlenecks["slow_functions"]:
            slow_count = len(bottlenecks["slow_functions"])
            recommendations.append(
                f"**ì„±ëŠ¥ ê°œì„ **: {slow_count}ê°œì˜ ëŠë¦° í•¨ìˆ˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. "
                "ë¹„ë™ê¸° ì²˜ë¦¬, ìºì‹±, ì•Œê³ ë¦¬ì¦˜ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
            )
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ê¶Œì¥ì‚¬í•­
        if bottlenecks["memory_intensive"]:
            recommendations.append(
                "**ë©”ëª¨ë¦¬ ìµœì í™”**: ë©”ëª¨ë¦¬ ì§‘ì•½ì  í•¨ìˆ˜ë“¤ì— ëŒ€í•´ "
                "ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬, ë°°ì¹˜ í¬ê¸° ì¡°ì •, ë©”ëª¨ë¦¬ í’€ ì‚¬ìš©ì„ ê²€í† í•˜ì„¸ìš”."
            )
        
        # CPU ìµœì í™” ê¶Œì¥ì‚¬í•­
        if bottlenecks["cpu_intensive"]:
            recommendations.append(
                "**CPU ìµœì í™”**: CPU ì§‘ì•½ì  ì‘ì—…ì— ëŒ€í•´ "
                "ë³‘ë ¬ ì²˜ë¦¬, ë²¡í„°í™”, Cython ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”."
            )
        
        # ìºì‹œ ìµœì í™” ê¶Œì¥ì‚¬í•­
        if bottlenecks["cache_inefficient"]:
            avg_hit_rate = np.mean([f["hit_rate"] for f in bottlenecks["cache_inefficient"]])
            recommendations.append(
                f"**ìºì‹œ ìµœì í™”**: í‰ê·  ìºì‹œ íˆíŠ¸ìœ¨ì´ {avg_hit_rate:.1%}ë¡œ ë‚®ìŠµë‹ˆë‹¤. "
                "ìºì‹œ í‚¤ ì „ëµ, TTL ì¡°ì •, ìºì‹œ í¬ê¸° ì¦ê°€ë¥¼ ê²€í† í•˜ì„¸ìš”."
            )
        
        # ì¼ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­
        recommendations.extend([
            "**ë¹„ë™ê¸° I/O**: íŒŒì¼ ë° ë„¤íŠ¸ì›Œí¬ ì‘ì—…ì— aiofilesì™€ aiohttp ì‚¬ìš©",
            "**ì—°ê²° í’€ë§**: ë°ì´í„°ë² ì´ìŠ¤ ë° HTTP ì—°ê²°ì— ëŒ€í•œ ì—°ê²° í’€ êµ¬í˜„",
            "**í”„ë¡œíŒŒì¼ë§**: ì •ê¸°ì ì¸ í”„ë¡œíŒŒì¼ë§ìœ¼ë¡œ ìƒˆë¡œìš´ ë³‘ëª©ì  ì¡°ê¸° ë°œê²¬",
            "**ëª¨ë‹ˆí„°ë§**: Prometheusì™€ Grafanaë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"
        ])
        
        return recommendations
    
    async def apply_automatic_optimizations(self):
        """ìë™ ìµœì í™” ì ìš©"""
        logger.info("ìë™ ìµœì í™” ì‹œì‘...")
        
        # 1. ë©”ëª¨ë¦¬ ìµœì í™”
        await self.optimize_memory_usage()
        
        # 2. ìºì‹œ ì •ë¦¬
        current_memory = psutil.virtual_memory().percent
        if current_memory > 70:
            await self._cleanup_caches()
        
        # 3. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        import gc
        gc.collect()
        
        # 4. í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ ì¡°ì •
        p = psutil.Process()
        p.nice(psutil.BELOW_NORMAL_PRIORITY_CLASS if os.name == 'nt' else 10)
        
        logger.info("ìë™ ìµœì í™” ì™„ë£Œ")
    
    async def save_performance_data(self, filepath: str):
        """ì„±ëŠ¥ ë°ì´í„° ì €ì¥"""
        data = {
            "metrics": dict(self.metrics),
            "optimization_history": list(self.optimization_history),
            "timestamp": time.time()
        }
        
        async with aiofiles.open(filepath, 'wb') as f:
            await f.write(pickle.dumps(data))
        
        logger.info(f"ì„±ëŠ¥ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
    
    async def load_performance_data(self, filepath: str):
        """ì„±ëŠ¥ ë°ì´í„° ë¡œë“œ"""
        try:
            async with aiofiles.open(filepath, 'rb') as f:
                data = pickle.loads(await f.read())
            
            self.metrics = defaultdict(lambda: PerformanceMetrics("", 0, 0, 0), data["metrics"])
            self.optimization_history = deque(data["optimization_history"], maxlen=1000)
            
            logger.info(f"ì„±ëŠ¥ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {filepath}")
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")


# ì „ì—­ ì„±ëŠ¥ ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤
performance_optimizer = PerformanceOptimizer()

# í¸ì˜ í•¨ìˆ˜ë“¤
performance_monitor = performance_optimizer.performance_monitor
optimized_cache = performance_optimizer.optimized_cache


async def run_performance_analysis():
    """ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰"""
    optimizer = performance_optimizer
    
    # ë³‘ëª©ì  ë¶„ì„
    bottlenecks = await optimizer.analyze_bottlenecks()
    
    # ë³´ê³ ì„œ ìƒì„±
    report = await optimizer.generate_optimization_report()
    
    # ë³´ê³ ì„œ ì €ì¥
    report_path = "performance_report.md"
    async with aiofiles.open(report_path, 'w', encoding='utf-8') as f:
        await f.write(report)
    
    print(f"ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_path}")
    
    # ìë™ ìµœì í™” ì ìš©
    await optimizer.apply_automatic_optimizations()
    
    return report


if __name__ == "__main__":
    asyncio.run(run_performance_analysis())